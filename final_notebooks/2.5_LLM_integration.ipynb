{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "464d2188",
   "metadata": {},
   "source": [
    "# LLM Integration Description\n",
    "LLM Integration refers to incorporation LLMs, in this case, Gemini, into businesses processes for enhanced efficiency, allowing for applications to leverage advanced NLP capabilites for a wide range of tasks.\n",
    "\n",
    "Some key concepts include...\n",
    "* API CAlls\n",
    "* Prompt Engineering\n",
    "* Data Handeling\n",
    "* RAG\n",
    "\n",
    "In this portion of the project, we are aiming to conduct LLM integration through: \n",
    "* Lead Scoring - identify and prioritize high value leads\n",
    "* Account Health - detects churn risks or upsell opportunities\n",
    "* Semantic Search - make chatbot retrieve and respond intelligently to business or sales data\n",
    "\n",
    "### Gemini: What is it?\n",
    "Gemini is an LLM developed by Google that allows for reasoning, code generation, and instruction following. \n",
    "\n",
    "### API - Application Programming Interface\n",
    "API will help us bridge between the LLM and the sent request. The API Key is a password that identifies the project when you use an APi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9261bdb7",
   "metadata": {},
   "source": [
    "## Step 1: Install Packages and Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fbf8b2b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2792048762.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpip install google-generativeai python-dotenv\u001b[39m\n        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#install Gemini\n",
    "pip install google-generativeai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9dc3b1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to Gemini API\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get the Gemini API key from .env\n",
    "api_key = os.getenv('GEMINI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "69ed107e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['models/embedding-gecko-001', 'models/gemini-2.5-pro-preview-03-25', 'models/gemini-2.5-flash-preview-05-20', 'models/gemini-2.5-flash', 'models/gemini-2.5-flash-lite-preview-06-17', 'models/gemini-2.5-pro-preview-05-06', 'models/gemini-2.5-pro-preview-06-05', 'models/gemini-2.5-pro', 'models/gemini-2.0-flash-exp', 'models/gemini-2.0-flash', 'models/gemini-2.0-flash-001', 'models/gemini-2.0-flash-exp-image-generation', 'models/gemini-2.0-flash-lite-001', 'models/gemini-2.0-flash-lite', 'models/gemini-2.0-flash-preview-image-generation', 'models/gemini-2.0-flash-lite-preview-02-05', 'models/gemini-2.0-flash-lite-preview', 'models/gemini-2.0-pro-exp', 'models/gemini-2.0-pro-exp-02-05', 'models/gemini-exp-1206', 'models/gemini-2.0-flash-thinking-exp-01-21', 'models/gemini-2.0-flash-thinking-exp', 'models/gemini-2.0-flash-thinking-exp-1219', 'models/gemini-2.5-flash-preview-tts', 'models/gemini-2.5-pro-preview-tts', 'models/learnlm-2.0-flash-experimental', 'models/gemma-3-1b-it', 'models/gemma-3-4b-it', 'models/gemma-3-12b-it', 'models/gemma-3-27b-it', 'models/gemma-3n-e4b-it', 'models/gemma-3n-e2b-it', 'models/gemini-flash-latest', 'models/gemini-flash-lite-latest', 'models/gemini-pro-latest', 'models/gemini-2.5-flash-lite', 'models/gemini-2.5-flash-image-preview', 'models/gemini-2.5-flash-image', 'models/gemini-2.5-flash-preview-09-2025', 'models/gemini-2.5-flash-lite-preview-09-2025', 'models/gemini-robotics-er-1.5-preview', 'models/gemini-2.5-computer-use-preview-10-2025', 'models/embedding-001', 'models/text-embedding-004', 'models/gemini-embedding-exp-03-07', 'models/gemini-embedding-exp', 'models/gemini-embedding-001', 'models/aqa', 'models/imagen-3.0-generate-002', 'models/imagen-4.0-generate-preview-06-06', 'models/imagen-4.0-ultra-generate-preview-06-06', 'models/imagen-4.0-generate-001', 'models/imagen-4.0-ultra-generate-001', 'models/imagen-4.0-fast-generate-001', 'models/veo-2.0-generate-001', 'models/veo-3.0-generate-preview', 'models/veo-3.0-fast-generate-preview', 'models/veo-3.0-generate-001', 'models/veo-3.0-fast-generate-001', 'models/veo-3.1-generate-preview', 'models/veo-3.1-fast-generate-preview', 'models/gemini-2.0-flash-live-001', 'models/gemini-live-2.5-flash-preview', 'models/gemini-2.5-flash-live-preview', 'models/gemini-2.5-flash-native-audio-latest', 'models/gemini-2.5-flash-native-audio-preview-09-2025']\n"
     ]
    }
   ],
   "source": [
    "#initialize Gemini and import necessary libraries\n",
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from __future__ import annotations\n",
    "from typing import Dict, Any, List, Optional\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "\n",
    "# Configure Gemini with your API key\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "# Debug: Print available models\n",
    "print(\"Available models:\", [m.name for m in genai.list_models()])\n",
    "\n",
    "# Create the model with correct name\n",
    "model = genai.GenerativeModel('models/gemini-pro-latest')  # Update model name to match available models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd468b3e",
   "metadata": {},
   "source": [
    "## Step 2: Create Helper Funcitons for Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c32f367e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GBDTLeadScorer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        pipeline=None,\n",
    "        features: Optional[List[str]] = None,\n",
    "        model_path: Optional[str] = None,\n",
    "        feature_path: Optional[str] = None,\n",
    "    ):\n",
    "        if pipeline is not None:\n",
    "            self.pipeline = pipeline\n",
    "        elif model_path and os.path.exists(model_path):\n",
    "            self.pipeline = joblib.load(model_path)\n",
    "        else:\n",
    "            raise ValueError(\"Provide a fitted pipeline or a valid model_path.\")\n",
    "\n",
    "        # Prefer explicit features arg; else try load from file; else None\n",
    "        if features is not None:\n",
    "            self.features = features\n",
    "        elif feature_path and os.path.exists(feature_path):\n",
    "            self.features = joblib.load(feature_path)\n",
    "        else:\n",
    "            self.features = None  # pipeline must handle missing order\n",
    "\n",
    "    def predict(self, features: Dict[str, Any]) -> float:\n",
    "        X = pd.DataFrame([features])\n",
    "        if self.features:\n",
    "            X = X.reindex(columns=self.features)\n",
    "        proba = self.pipeline.predict_proba(X)[:, 1]\n",
    "        return float(proba[0])\n",
    "\n",
    "\n",
    "class AccountHealthScorer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        pipeline=None,\n",
    "        features: Optional[List[str]] = None,\n",
    "        model_path: Optional[str] = None,\n",
    "        feature_path: Optional[str] = None,\n",
    "    ):\n",
    "        if pipeline is not None:\n",
    "            self.pipeline = pipeline\n",
    "        elif model_path and os.path.exists(model_path):\n",
    "            self.pipeline = joblib.load(model_path)\n",
    "        else:\n",
    "            raise ValueError(\"Provide a fitted pipeline or a valid model_path.\")\n",
    "\n",
    "        if features is not None:\n",
    "            self.features = features\n",
    "        elif feature_path and os.path.exists(feature_path):\n",
    "            self.features = joblib.load(feature_path)\n",
    "        else:\n",
    "            self.features = None\n",
    "\n",
    "    def predict(self, features: Dict[str, Any]) -> float:\n",
    "        X = pd.DataFrame([features])\n",
    "        if self.features:\n",
    "            X = X.reindex(columns=self.features)\n",
    "        proba = self.pipeline.predict_proba(X)[:, 1]  # adjust if needed\n",
    "        return float(proba[0])\n",
    "\n",
    "\n",
    "class SemanticSearcher:\n",
    "    \"\"\"Use an existing Chroma collection/client if you already created one.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        collection=None,\n",
    "        client=None,\n",
    "        persist_dir: Optional[str] = None,\n",
    "        collection_name: str = \"crm_docs\",\n",
    "    ):\n",
    "        if collection is not None:\n",
    "            self.collection = collection\n",
    "        else:\n",
    "            if client is None:\n",
    "                if not persist_dir:\n",
    "                    raise ValueError(\"Provide collection/client OR persist_dir.\")\n",
    "                import chromadb\n",
    "                client = chromadb.PersistentClient(path=persist_dir)\n",
    "            self.collection = client.get_or_create_collection(collection_name)\n",
    "\n",
    "    def search(self, query: str, n_results: int = 5, where: Optional[Dict[str, Any]] = None):\n",
    "        res = self.collection.query(query_texts=[query], n_results=n_results, where=where)\n",
    "        docs = res.get(\"documents\", [[]])[0]\n",
    "        metas = res.get(\"metadatas\", [[]])[0]\n",
    "        dists = res.get(\"distances\", [[]])[0] if \"distances\" in res else [None]*len(docs)\n",
    "        out = []\n",
    "        for i, txt in enumerate(docs):\n",
    "            meta = metas[i] if i < len(metas) else {}\n",
    "            out.append({\n",
    "                \"text\": txt,\n",
    "                \"source\": (meta or {}).get(\"source\", f\"doc_{i}\"),\n",
    "                \"score\": dists[i]\n",
    "            })\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f560755f",
   "metadata": {},
   "source": [
    "## Step 3: Create Main Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f639ec6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"GEMINI_API_KEY not set in your .env\")\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "class CRMAssistant:\n",
    "    def __init__(self, lead_scorer, health_scorer, semantic_searcher, model_name=\"gemini-pro\"):\n",
    "        self.lead_scorer = lead_scorer\n",
    "        self.health_scorer = health_scorer\n",
    "        self.semantic_searcher = semantic_searcher\n",
    "        self.model = genai.GenerativeModel(model_name)\n",
    "\n",
    "    @staticmethod\n",
    "    def _fmt(docs: List[Dict[str, Any]]) -> str:\n",
    "        return \"\\n\".join(f\"[{d.get('source','doc')}] {d.get('text','')}\" for d in docs)\n",
    "\n",
    "    def process_lead_score(self, lead_score: float, lead_data: Dict[str, Any]) -> str:\n",
    "        prompt = f\"\"\"\n",
    "You are a CRM assistant. Given a lead score of {lead_score:.2f} and this lead data:\n",
    "{lead_data}\n",
    "\n",
    "Provide:\n",
    "- 2–4 next actions,\n",
    "- a short rationale referencing top drivers,\n",
    "- a one-line priority (High/Med/Low).\n",
    "\"\"\"\n",
    "        return self.model.generate_content(prompt).text\n",
    "\n",
    "    def analyze_account_health(self, health_score: float, account_data: Dict[str, Any]) -> str:\n",
    "        prompt = f\"\"\"\n",
    "You are a CRM assistant. Account health score: {health_score:.2f}.\n",
    "Account data:\n",
    "{account_data}\n",
    "\n",
    "Return:\n",
    "- 3 targeted recommendations,\n",
    "- key risks,\n",
    "- owner + due date for the first action.\n",
    "\"\"\"\n",
    "        return self.model.generate_content(prompt).text\n",
    "\n",
    "    def semantic_search_response(self, query: str, context_docs: List[Dict[str, Any]]) -> str:\n",
    "        ctx = self._fmt(context_docs)\n",
    "        prompt = f\"\"\"\n",
    "Use the CRM context to answer. If info is missing, say what is needed.\n",
    "\n",
    "Context:\n",
    "{ctx}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Reply with a brief summary and bullet points. Cite sources in [brackets].\n",
    "\"\"\"\n",
    "        return self.model.generate_content(prompt).text\n",
    "\n",
    "    def process_query(self, query: str, context: Optional[Dict[str, Any]] = None) -> str:\n",
    "        q = query.lower()\n",
    "        if \"lead\" in q and any(k in q for k in [\"score\", \"convert\", \"probability\"]):\n",
    "            if context is None:\n",
    "                return \"I need lead features (or a lead_id I can fetch) to score this lead.\"\n",
    "            score = self.lead_scorer.predict(context)\n",
    "            return self.process_lead_score(score, context)\n",
    "\n",
    "        if any(k in q for k in [\"health\", \"churn\", \"risk\", \"renewal\"]):\n",
    "            if context is None:\n",
    "                return \"I need account features (or an account_id I can fetch) to assess health.\"\n",
    "            score = self.health_scorer.predict(context)\n",
    "            return self.analyze_account_health(score, context)\n",
    "\n",
    "        docs = self.semantic_searcher.search(query)\n",
    "        return self.semantic_search_response(query, docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0759ddb4",
   "metadata": {},
   "source": [
    "## Step 4: Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1b03caa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing Gemini LLM Integration ===\n",
      "\n",
      "Prompt: What are the key benefits of using LLMs in CRM systems?\n",
      "\n",
      "Response: Of course. Integrating Large Language Models (LLMs) into Customer Relationship Management (CRM) systems is a transformative shift, moving CRMs from being a passive \"system of record\" to a proactive \"system of intelligence.\"\n",
      "\n",
      "Here are the key benefits, broken down by the core functions of a CRM:\n",
      "\n",
      "---\n",
      "\n",
      "### 1. For Sales Teams (Sales Enablement & Productivity)\n",
      "\n",
      "This is arguably the area with the most immediate and tangible impact. LLMs act as a \"co-pilot\" for sales representatives.\n",
      "\n",
      "*   **Automated and Hyper-Personalized Outreach:** Instead of reps spending hours writing emails, LLMs can instantly draft highly personalized outreach messages.\n",
      "    *   **How it works:** The LLM analyzes the contact's LinkedIn profile, past interactions, company news, and deal information within the CRM to generate a relevant, context-aware email or message.\n",
      "    *   **Benefit:** Massive time savings, higher quality communication, and increased response rates.\n",
      "\n",
      "*   **Intelligent Summarization:** Sales reps are often buried in information from calls, meetings, and long email chains.\n",
      "    *   **How it works:** An LLM can automatically transcribe a sales call and generate a concise, bulleted summary highlighting key customer pain points, action items, and next steps. This summary is then automatically logged in the CRM.\n",
      "    *   **Benefit:** Eliminates manual note-taking, ensures key details are never missed, and makes it easy for managers or other team members to get up to speed on a deal instantly.\n",
      "\n",
      "*   **Real-time Coaching and \"Next Best Action\" Suggestions:** LLMs can guide reps on what to do next to move a deal forward.\n",
      "    *   **How it works:** By analyzing the entire history of a deal (emails, calls, stage duration), the LLM can identify risks or opportunities. It might suggest, \"This deal has been stalled for 10 days. Suggest sending a case study about [relevant topic]\" or \"The prospect mentioned competitor X; here is a battle card to handle that objection.\"\n",
      "    *   **Benefit:** Improves sales effectiveness, shortens sales cycles, and helps new reps ramp up faster.\n",
      "\n",
      "*   **Automated Data Entry:** A major pain point for all sales reps.\n",
      "    *   **How it works:** The LLM can parse an incoming email and automatically update the CRM. For example, it can identify a new contact person and add them, update a deal stage, or log the communication activity.\n",
      "    *   **Benefit:** Frees up significant administrative time, improves data accuracy, and ensures the CRM is always up-to-date.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. For Customer Service & Support Teams\n",
      "\n",
      "LLMs empower support agents to resolve issues faster and provide a better customer experience.\n",
      "\n",
      "*   **Next-Generation Chatbots & Virtual Agents:** Traditional chatbots are rigid and frustrating. LLM-powered bots are conversational and far more capable.\n",
      "    *   **How it works:** They can understand complex, natural language queries, access the company's knowledge base, and provide detailed, helpful answers or even perform actions (like checking an order status) without human intervention.\n",
      "    *   **Benefit:** 24/7 support, instant resolution for common issues, and reduced ticket volume for human agents, allowing them to focus on more complex problems.\n",
      "\n",
      "*   **Agent Assist & Co-pilot:** LLMs act as a real-time assistant for human agents.\n",
      "    *   **How it works:** While an agent is on a call or chat, the LLM can listen in, understand the customer's issue, and automatically surface relevant knowledge base articles, past ticket information, or step-by-step resolution guides on the agent's screen. It can also draft empathetic and accurate responses for the agent to review and send.\n",
      "    *   **Benefit:** Drastically reduces average handling time (AHT), improves first-contact resolution rates, and ensures consistent service quality.\n",
      "\n",
      "*   **Sentiment Analysis and Trend Identification:** LLMs can read and understand the emotion and intent behind customer communications at scale.\n",
      "    *   **How it works:** The LLM can analyze thousands of support tickets, surveys, and call transcripts to identify the overall sentiment (positive, negative, neutral) and pinpoint emerging issues or common complaints (e.g., \"Many customers in Germany are complaining about a bug in our latest software update\").\n",
      "    *   **Benefit:** Provides an early warning system for product or service issues, helps prioritize improvements, and reduces customer churn.\n",
      "\n",
      "---\n",
      "\n",
      "### 3. For Marketing Teams (Automation & Personalization)\n",
      "\n",
      "LLMs enable marketers to create more effective campaigns with less effort.\n",
      "\n",
      "*   **Dynamic Content Generation at Scale:** Marketers need to create vast amounts of content for different channels and segments.\n",
      "    *   **How it works:** An LLM can generate variations of ad copy, email subject lines, social media posts, and even blog content tailored to specific customer segments defined in the CRM.\n",
      "    *   **Benefit:** Accelerates content creation, enables A/B testing on a massive scale, and delivers more personalized marketing messages.\n",
      "\n",
      "*   **Advanced Audience Segmentation:** LLMs can uncover new customer segments based on unstructured data.\n",
      "    *   **How it works:** It can analyze open-ended survey responses, product reviews, and support ticket notes to group customers based on their motivations, pain points, or use cases—insights that are impossible to get from structured data alone.\n",
      "    *   **Benefit:** Leads to more sophisticated and effective targeting for marketing campaigns.\n",
      "\n",
      "*   **Smarter Lead Scoring and Nurturing:** LLMs can interpret the nuances of a lead's engagement.\n",
      "    *   **How it works:** Instead of just scoring a lead based on opening an email, an LLM can analyze the *content* of their reply to gauge interest, urgency, and intent, providing a much more accurate lead score. It can then suggest the perfect piece of content for the next step in the nurture sequence.\n",
      "    *   **Benefit:** Improves the quality of leads passed to sales and increases conversion rates.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. For Business Operations & Analytics\n",
      "\n",
      "*   **Conversational Analytics and Reporting:** This is a game-changer for data accessibility.\n",
      "    *   **How it works:** Instead of building complex reports, a manager can simply ask the CRM a question in plain English, like: \"Show me my team's pipeline conversion rate for last quarter compared to the previous one\" or \"Which lead source generated the most revenue in North America this year?\"\n",
      "    *   **Benefit:** Democratizes data access, allowing non-technical users to get powerful insights instantly without needing a data analyst.\n",
      "\n",
      "*   **Improved Data Hygiene:** LLMs can help clean and maintain the quality of CRM data.\n",
      "    *   **How it works:** They can identify duplicate records with slight variations (e.g., \"Bob Smith\" vs. \"Robert Smith\"), standardize formatting, and enrich records by finding missing information from public sources.\n",
      "    *   **Benefit:** Increases the reliability and value of the CRM as the single source of truth for customer data.\n",
      "\n",
      "### Summary Table\n",
      "\n",
      "| Benefit Area | Key Benefit | Impact on the Business |\n",
      "| :--- | :--- | :--- |\n",
      "| **Sales** | **AI Co-pilot** | Increased rep productivity, shorter sales cycles, higher win rates. |\n",
      "| **Sales** | **Automated Summarization** | Better data capture, improved team collaboration, time savings. |\n",
      "| **Customer Service** | **Intelligent Virtual Agents** | Lower support costs, 24/7 availability, improved customer satisfaction. |\n",
      "| **Customer Service** | **Agent Assist** | Faster ticket resolution, higher agent satisfaction, consistent service. |\n",
      "| **Marketing** | **Content Generation** | Accelerated campaign creation, hyper-personalization at scale. |\n",
      "| **Marketing** | **Advanced Segmentation** | More effective targeting, higher campaign ROI. |\n",
      "| **Operations** | **Conversational Analytics** | Faster, data-driven decision-making for everyone. |\n",
      "| **Operations** | **Automated Data Entry** | Improved data quality, massive time savings for all users. |\n",
      "\n",
      "In short, integrating LLMs transforms a CRM from a static database into a dynamic, intelligent partner that actively helps every employee be more productive and effective in their role.\n",
      "\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "# Basic LLM Integration Test\n",
    "# Test function remains the same\n",
    "def test_llm_integration():\n",
    "    prompt = \"What are the key benefits of using LLMs in CRM systems?\"\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config={\n",
    "                \"temperature\": 0.7,\n",
    "                \"top_p\": 0.8,\n",
    "                \"top_k\": 40\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        print(\"\\n=== Testing Gemini LLM Integration ===\")\n",
    "        print(\"\\nPrompt:\", prompt)\n",
    "        print(\"\\nResponse:\", response.text)\n",
    "        print(\"\\n=====================================\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error testing LLM integration: {str(e)}\")\n",
    "        print(\"API Key configured:\", bool(api_key))\n",
    "        print(\"Available models:\", [m.name for m in genai.list_models()])\n",
    "\n",
    "# Run the test\n",
    "test_llm_integration()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
