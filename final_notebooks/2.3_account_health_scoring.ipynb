{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af8a272f",
   "metadata": {},
   "source": [
    "# Account Health Scoring Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60e35d3",
   "metadata": {},
   "source": [
    "Account health scoring predicts how **valuable/engaged/at-risk** a customer account is on a scale of 0-100. It's like a \"wellness score\" for customer relationships:\n",
    "- High score (70-100): Healthy, engaged, likely to renew/expand\n",
    "- Medium score (40-69): Stable but needs attention\n",
    "- Low score (0-39): At-risk, potential churn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c48081",
   "metadata": {},
   "source": [
    "## Step 1: Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1da1eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88e9f8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7ed1b2",
   "metadata": {},
   "source": [
    "## Step 2: Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64d8cc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.path.abspath(\"../data_directory/clean_data\")\n",
    "\n",
    "# Read the data\n",
    "pipeline = pd.read_csv(os.path.join(base_path, \"Pipeline.csv\"))\n",
    "accounts = pd.read_csv(os.path.join(base_path, \"Accounts.csv\"))\n",
    "teams = pd.read_csv(os.path.join(base_path, \"Teams.csv\"))\n",
    "products = pd.read_csv(os.path.join(base_path, \"Products.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30ccf0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCOUNTS: (85, 7)\n",
      "['account', 'sector', 'year_established', 'revenue', 'employees', 'office_location', 'subsidiary_of']\n",
      "            account      sector  year_established  revenue  employees  \\\n",
      "0  acme_corporation  technology            1996.0  1100.04     2822.0   \n",
      "1        betasoloin     medical            1999.0   251.41      495.0   \n",
      "2          betatech     medical            1986.0   647.18     1185.0   \n",
      "\n",
      "  office_location     subsidiary_of  \n",
      "0   united_states  acme_corporation  \n",
      "1   united_states  acme_corporation  \n",
      "2           kenya  acme_corporation  \n",
      "\n",
      "PIPELINE: (8800, 8)\n",
      "['opportunity_id', 'sales_agent', 'product', 'account', 'deal_stage', 'engage_date', 'close_date', 'close_value']\n",
      "  opportunity_id      sales_agent         product  account deal_stage  \\\n",
      "0       1c1i7a6r      moses_frase  gtx_plus_basic  cancity        won   \n",
      "1       z063oyw0  darcel_schlecht          gtxpro    isdom        won   \n",
      "2       ec4qe1bx  darcel_schlecht      mg_special  cancity        won   \n",
      "\n",
      "  engage_date  close_date  close_value  \n",
      "0  2016-10-20  2017-03-01       1054.0  \n",
      "1  2016-10-25  2017-03-11       4514.0  \n",
      "2  2016-10-25  2017-03-07         50.0  \n"
     ]
    }
   ],
   "source": [
    "# Understanding each dataset\n",
    "print(\"ACCOUNTS:\", accounts.shape)\n",
    "print(accounts.columns.tolist())\n",
    "print(accounts.head(3))\n",
    "print(\"\\nPIPELINE:\", pipeline.shape)\n",
    "print(pipeline.columns.tolist())\n",
    "print(pipeline.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e470294",
   "metadata": {},
   "source": [
    "## Step 3: Create engagement metrics from Pipeline data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "052da13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge pipeline with products to get revenue info\n",
    "pipeline_enhanced = pipeline.merge(products, on='product', how='left')\n",
    "\n",
    "# Calculate per-account metrics\n",
    "account_metrics = pipeline.groupby('account').agg({\n",
    "    'opportunity_id': 'count',  # Total opportunities\n",
    "    'close_value': ['sum', 'mean', 'std', 'max'],  # Revenue metrics + max deal\n",
    "    'deal_stage': [\n",
    "        lambda x: (x == 'Won').sum(),      # Win count\n",
    "        lambda x: (x == 'Lost').sum(),     # Loss count\n",
    "        'count'                             # Total deals\n",
    "    ],\n",
    "    'engage_date': lambda x: (pd.to_datetime(x).max() - pd.to_datetime(x).min()).days,  # Customer lifetime\n",
    "    'product': 'nunique',       # Product diversity\n",
    "    'sales_agent': 'nunique'    # Agent diversity\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "459b921e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten column names\n",
    "account_metrics.columns = [\n",
    "    'account', \n",
    "    'total_opportunities', \n",
    "    'total_revenue', \n",
    "    'avg_deal_size', \n",
    "    'revenue_std', \n",
    "    'max_deal_size',       \n",
    "    'total_wins', \n",
    "    'total_losses',        \n",
    "    'total_deals',         \n",
    "    'customer_lifetime_days',  # NEW\n",
    "    'product_diversity', \n",
    "    'agent_count'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75ee4767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the new metrics\n",
    "account_metrics['win_rate'] = (account_metrics['total_wins'] /  account_metrics['total_opportunities'] * 100)\n",
    "\n",
    "account_metrics['loss_rate'] = (account_metrics['total_losses'] /  account_metrics['total_opportunities'] * 100)  # NEW\n",
    "\n",
    "account_metrics['deals_per_year'] = (account_metrics['total_deals'] / (account_metrics['customer_lifetime_days'] / 365 + 0.01))  # NEW (can't divide by zero)\n",
    "\n",
    "# Merge with accounts data\n",
    "merged_df = accounts.merge(account_metrics, on='account', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc909acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "days_since_last_deal NaN count: 0\n"
     ]
    }
   ],
   "source": [
    "# Days since last deal\n",
    "last_deal_dates = pipeline.groupby('account')['close_date'].max().reset_index()\n",
    "last_deal_dates.columns = ['account', 'last_deal_date']\n",
    "\n",
    "# Merge properly\n",
    "merged_df = merged_df.merge(last_deal_dates, on='account', how='left')\n",
    "merged_df['days_since_last_deal'] = (pd.Timestamp.now() - pd.to_datetime(merged_df['last_deal_date'])).dt.days\n",
    "\n",
    "# Drop the temporary column\n",
    "merged_df.drop('last_deal_date', axis=1, inplace=True)\n",
    "\n",
    "print(f\"days_since_last_deal NaN count: {merged_df['days_since_last_deal'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e19ed1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nl/6y23jbvx6976ljxjh0vqgr7w0000gn/T/ipykernel_86413/1984320299.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['recent_revenue'].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Revenue momentum (recent performance vs historical)\n",
    "recent_cutoff = '2016-01-01'  # Guesstimate\n",
    "recent_revenue = (pipeline[pd.to_datetime(pipeline['close_date']) > recent_cutoff]\n",
    "                  .groupby('account')['close_value'].sum())\n",
    "merged_df = merged_df.merge(recent_revenue.rename('recent_revenue'), \n",
    "                            on='account', how='left')\n",
    "merged_df['recent_revenue'].fillna(0, inplace=True)\n",
    "merged_df['revenue_momentum'] = merged_df['recent_revenue'] / (merged_df['total_revenue'] + 1)  # +1 to avoid division by zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7682329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataframe shape: (85, 24)\n",
      "New features added: 24 total columns\n",
      "\n",
      "Column names:\n",
      "['account', 'sector', 'year_established', 'revenue', 'employees', 'office_location', 'subsidiary_of', 'total_opportunities', 'total_revenue', 'avg_deal_size', 'revenue_std', 'max_deal_size', 'total_wins', 'total_losses', 'total_deals', 'customer_lifetime_days', 'product_diversity', 'agent_count', 'win_rate', 'loss_rate', 'deals_per_year', 'days_since_last_deal', 'recent_revenue', 'revenue_momentum']\n",
      "\n",
      "First few rows:\n",
      "            account      sector  year_established  revenue  employees  \\\n",
      "0  acme_corporation  technology            1996.0  1100.04     2822.0   \n",
      "1        betasoloin     medical            1999.0   251.41      495.0   \n",
      "2          betatech     medical            1986.0   647.18     1185.0   \n",
      "3        bioholding     medical            2012.0   587.34     1356.0   \n",
      "4           bioplex     medical            1991.0   326.82     1016.0   \n",
      "\n",
      "  office_location     subsidiary_of  total_opportunities  total_revenue  \\\n",
      "0   united_states  acme_corporation                   68       106464.0   \n",
      "1   united_states  acme_corporation                   68       103644.0   \n",
      "2           kenya  acme_corporation                   92       110240.0   \n",
      "3     philippines  acme_corporation                   94        95239.0   \n",
      "4   united_states  acme_corporation                   53        70697.0   \n",
      "\n",
      "   avg_deal_size  ...  total_deals  customer_lifetime_days  product_diversity  \\\n",
      "0    1565.647059  ...           68                     377                  6   \n",
      "1    1524.176471  ...           68                     392                  7   \n",
      "2    1198.260870  ...           92                     383                  6   \n",
      "3    1013.180851  ...           94                     410                  6   \n",
      "4    1333.905660  ...           53                     388                  6   \n",
      "\n",
      "   agent_count  win_rate  loss_rate  deals_per_year  days_since_last_deal  \\\n",
      "0           16       0.0        0.0       65.204256                  2857   \n",
      "1           19       0.0        0.0       62.732213                  2856   \n",
      "2           13       0.0        0.0       86.848571                  2855   \n",
      "3           13       0.0        0.0       82.944518                  2854   \n",
      "4           14       0.0        0.0       49.393591                  2868   \n",
      "\n",
      "   recent_revenue  revenue_momentum  \n",
      "0        106464.0          0.999991  \n",
      "1        103644.0          0.999990  \n",
      "2        110240.0          0.999991  \n",
      "3         95239.0          0.999990  \n",
      "4         70697.0          0.999986  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"Merged dataframe shape:\", merged_df.shape)\n",
    "print(f\"New features added: {merged_df.shape[1]} total columns\")\n",
    "print(\"\\nColumn names:\")\n",
    "print(merged_df.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1edea29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in new features:\n",
      "max_deal_size             0\n",
      "total_losses              0\n",
      "customer_lifetime_days    0\n",
      "loss_rate                 0\n",
      "deals_per_year            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMissing values in new features:\")\n",
    "print(merged_df[['max_deal_size', 'total_losses', 'customer_lifetime_days', \n",
    "                  'loss_rate', 'deals_per_year']].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c4bf05",
   "metadata": {},
   "source": [
    "## Step 4: Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8237ada5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to normalize everything to 0-100 scale\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1aef12e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target range: 3.31 to 55.00\n",
      "Target mean: 12.07\n",
      "Target distribution:\n",
      "count    85.000000\n",
      "mean     12.066693\n",
      "std       6.149092\n",
      "min       3.309287\n",
      "25%       9.079234\n",
      "50%      10.920000\n",
      "75%      13.961184\n",
      "max      55.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 100))\n",
    "\n",
    "# Components of health score\n",
    "components = pd.DataFrame()\n",
    "components['revenue_score'] = scaler.fit_transform(merged_df[['total_revenue']])[:, 0]\n",
    "components['win_rate_score'] = merged_df['win_rate']  # Already 0-100\n",
    "components['engagement_score'] = scaler.fit_transform(merged_df[['total_opportunities']])\n",
    "components['avg_deal_score'] = scaler.fit_transform(merged_df[['avg_deal_size']])\n",
    "\n",
    "# Weighted combination (ask JP and Fanizza if this makes sense)\n",
    "weights = {\n",
    "    'revenue_score': 0.3,      # 30% (reduced from 50%)\n",
    "    'win_rate_score': 0.3,     # 30% (increased from 25%)\n",
    "    'engagement_score': 0.25,  # 25% (increased from 15%)\n",
    "    'avg_deal_score': 0.15     # 15% (increased from 10%)\n",
    "}\n",
    "\n",
    "y_target = (\n",
    "    components['revenue_score'] * weights['revenue_score'] +\n",
    "    components['win_rate_score'] * weights['win_rate_score'] +\n",
    "    components['engagement_score'] * weights['engagement_score'] +\n",
    "    components['avg_deal_score'] * weights['avg_deal_score']\n",
    ")\n",
    "\n",
    "# Verify it's 0-100\n",
    "print(f\"Target range: {y_target.min():.2f} to {y_target.max():.2f}\")\n",
    "print(f\"Target mean: {y_target.mean():.2f}\")\n",
    "print(f\"Target distribution:\\n{y_target.describe()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2666b7e7",
   "metadata": {},
   "source": [
    "## Preparing the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d613924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "merged_df['has_subsidiary'] = (merged_df['subsidiary_of'] != 'Independent').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "497d733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode sector and location\n",
    "merged_df = pd.get_dummies(merged_df, columns=['sector', 'office_location'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9335d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features for model\n",
    "feature_cols = [\n",
    "    # Account characteristics\n",
    "    'revenue', \n",
    "    'employees', \n",
    "    'year_established', \n",
    "    'has_subsidiary',\n",
    "    \n",
    "    # Engagement features\n",
    "    'product_diversity', \n",
    "    'agent_count',\n",
    "    'customer_lifetime_days',\n",
    "    'loss_rate',\n",
    "    'days_since_last_deal',\n",
    "    'deals_per_year',\n",
    "    'max_deal_size'\n",
    "] + [col for col in merged_df.columns if col.startswith('sector_') or \n",
    "     col.startswith('office_location_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74d25fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values before cleaning:\n",
      "0\n",
      "Features shape: (85, 34)\n",
      "Feature columns: ['revenue', 'employees', 'year_established', 'has_subsidiary', 'product_diversity', 'agent_count', 'customer_lifetime_days', 'loss_rate', 'days_since_last_deal', 'deals_per_year', 'max_deal_size', 'sector_entertainment', 'sector_finance', 'sector_marketing', 'sector_medical', 'sector_retail', 'sector_services', 'sector_software', 'sector_technology', 'sector_telecommunications', 'office_location_brazil', 'office_location_china', 'office_location_germany', 'office_location_italy', 'office_location_japan', 'office_location_jordan', 'office_location_kenya', 'office_location_korea', 'office_location_norway', 'office_location_panama', 'office_location_philippines', 'office_location_poland', 'office_location_romania', 'office_location_united_states']\n"
     ]
    }
   ],
   "source": [
    "X = merged_df[feature_cols]\n",
    "y = y_target\n",
    "\n",
    "print(\"Missing values before cleaning:\")\n",
    "print(X.isnull().sum().sum())\n",
    "\n",
    "# Replace inf with NaN first\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Fill NaN with median\n",
    "X = X.fillna(X.median())\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Feature columns: {X.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31595b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after cleaning:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Double check\n",
    "print(\"Missing values after cleaning:\")\n",
    "print(X.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1248ea",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fd1bcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ebfd60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88fcd1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the top 3 scaling methods\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge': Ridge(alpha=1.0), # From JP's notes\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10)\n",
    "}\n",
    "\n",
    "scaling_methods = ['normalize', 'clipping', 'direct']  # Starting w/ just with these 3\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f906143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models.items():\n",
    "    for scaling in scaling_methods:\n",
    "        # Train model\n",
    "        if scaling == 'normalize':\n",
    "            y_train_scaled = y_train / 100\n",
    "            model.fit(X_train_scaled, y_train_scaled)\n",
    "            preds = model.predict(X_test_scaled) * 100\n",
    "        else:\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            preds = model.predict(X_test_scaled)\n",
    "        \n",
    "        # Apply scaling\n",
    "        if scaling == 'clipping':\n",
    "            preds = np.clip(preds, 0, 100)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        r2 = r2_score(y_test, preds) # Measures overall model fit\n",
    "        mae = mean_absolute_error(y_test, preds) # More of a general error measurement\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, preds)) # Penalizes large errors more than MAE\n",
    "        \n",
    "        results[f\"{model_name}_{scaling}\"] = {\n",
    "            'R2': round(r2, 4),\n",
    "            'MAE': round(mae, 2),\n",
    "            'RMSE': round(rmse, 2)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c91262e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MODEL RESULTS\n",
      "                                 R2   MAE  RMSE\n",
      "Ridge_normalize              0.8792  2.91  3.71\n",
      "Ridge_clipping               0.8792  2.91  3.71\n",
      "Ridge_direct                 0.8792  2.91  3.71\n",
      "Linear Regression_normalize  0.8751  3.02  3.77\n",
      "Linear Regression_clipping   0.8751  3.02  3.77\n",
      "Linear Regression_direct     0.8751  3.02  3.77\n",
      "Random Forest_clipping       0.1696  4.15  9.73\n",
      "Random Forest_direct         0.1696  4.15  9.73\n",
      "Random Forest_normalize      0.1587  4.11  9.79\n"
     ]
    }
   ],
   "source": [
    "# Show results\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"\\nMODEL RESULTS\")\n",
    "print(results_df.sort_values('R2', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee40b43",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c855a05",
   "metadata": {},
   "source": [
    "After evaluating, the best model is ridge regression! It has the **highest R2 score** of 0.8792 (explains the most variance). That means it explains about 88% of the variance in account health scores and it is above the target of 70%. The model’s MAE is 2.91, meaning the average prediction error is under 3 points on a 0–100 scale, and the RMSE is 3.71, so large errors are pretty rare.\n",
    "\n",
    "Ridge Regression slightly outperformed regular Linear Regression (0.8792 vs 0.8751), which shows how Ridge’s regularization helps handle feature relationships better. From my testing and research, I saw that the regularization keeps the model from overfitting while still making stable predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1cf096",
   "metadata": {},
   "source": [
    "Interestingly, all three scaling methods (normalize, clipping, and direct) gave identical results for both Ridge and Linear Regression. That makes sense since linear models are scale-invariant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b7904e",
   "metadata": {},
   "source": [
    "Random forest was the worst which I think is because the dataset is too small for that model. My research showed that Random Forests usually need 1,000+ samples to detect meaningful patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b85942",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
