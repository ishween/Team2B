{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af8a272f",
   "metadata": {},
   "source": [
    "# Account Health Scoring Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60e35d3",
   "metadata": {},
   "source": [
    "Account health scoring predicts how **valuable/engaged/at-risk** a customer account is on a scale of 0-100. It's like a \"wellness score\" for customer relationships:\n",
    "- High score (70-100): Healthy, engaged, likely to renew/expand\n",
    "- Medium score (40-69): Stable but needs attention\n",
    "- Low score (0-39): At-risk, potential churn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c48081",
   "metadata": {},
   "source": [
    "## Step 1: Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1da1eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88e9f8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7ed1b2",
   "metadata": {},
   "source": [
    "## Step 2: Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64d8cc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.path.abspath(\"../data_directory/clean_data\")\n",
    "\n",
    "# Read the data\n",
    "pipeline = pd.read_csv(os.path.join(base_path, \"Pipeline.csv\"))\n",
    "accounts = pd.read_csv(os.path.join(base_path, \"Accounts.csv\"))\n",
    "teams = pd.read_csv(os.path.join(base_path, \"Teams.csv\"))\n",
    "products = pd.read_csv(os.path.join(base_path, \"Products.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30ccf0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCOUNTS: (85, 7)\n",
      "['account', 'sector', 'year_established', 'revenue', 'employees', 'office_location', 'subsidiary_of']\n",
      "            account      sector  year_established  revenue  employees  \\\n",
      "0  acme_corporation  technology            1996.0  1100.04     2822.0   \n",
      "1        betasoloin     medical            1999.0   251.41      495.0   \n",
      "2          betatech     medical            1986.0   647.18     1185.0   \n",
      "\n",
      "  office_location     subsidiary_of  \n",
      "0   united_states  acme_corporation  \n",
      "1   united_states  acme_corporation  \n",
      "2           kenya  acme_corporation  \n",
      "\n",
      "PIPELINE: (8800, 8)\n",
      "['opportunity_id', 'sales_agent', 'product', 'account', 'deal_stage', 'engage_date', 'close_date', 'close_value']\n",
      "  opportunity_id      sales_agent         product  account deal_stage  \\\n",
      "0       1c1i7a6r      moses_frase  gtx_plus_basic  cancity        won   \n",
      "1       z063oyw0  darcel_schlecht          gtxpro    isdom        won   \n",
      "2       ec4qe1bx  darcel_schlecht      mg_special  cancity        won   \n",
      "\n",
      "  engage_date  close_date  close_value  \n",
      "0  2016-10-20  2017-03-01       1054.0  \n",
      "1  2016-10-25  2017-03-11       4514.0  \n",
      "2  2016-10-25  2017-03-07         50.0  \n"
     ]
    }
   ],
   "source": [
    "# Understanding each dataset\n",
    "print(\"ACCOUNTS:\", accounts.shape)\n",
    "print(accounts.columns.tolist())\n",
    "print(accounts.head(3))\n",
    "print(\"\\nPIPELINE:\", pipeline.shape)\n",
    "print(pipeline.columns.tolist())\n",
    "print(pipeline.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e470294",
   "metadata": {},
   "source": [
    "## Step 3: Create engagement metrics from Pipeline data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "052da13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge pipeline with products to get revenue info\n",
    "pipeline_enhanced = pipeline.merge(products, on='product', how='left')\n",
    "\n",
    "# Calculate per-account metrics\n",
    "account_metrics = pipeline.groupby('account').agg({\n",
    "    'opportunity_id': 'count',  # Total opportunities\n",
    "    'close_value': ['sum', 'mean', 'std', 'max'],  # Revenue metrics + max deal\n",
    "    'deal_stage': [\n",
    "        lambda x: (x == 'Won').sum(),      # Win count\n",
    "        lambda x: (x == 'Lost').sum(),     # Loss count\n",
    "        'count'                             # Total deals\n",
    "    ],\n",
    "    'engage_date': lambda x: (pd.to_datetime(x).max() - pd.to_datetime(x).min()).days,  # Customer lifetime\n",
    "    'product': 'nunique',       # Product diversity\n",
    "    'sales_agent': 'nunique'    # Agent diversity\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "459b921e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten column names\n",
    "account_metrics.columns = [\n",
    "    'account', \n",
    "    'total_opportunities', \n",
    "    'total_revenue', \n",
    "    'avg_deal_size', \n",
    "    'revenue_std', \n",
    "    'max_deal_size',           # NEW\n",
    "    'total_wins', \n",
    "    'total_losses',            # NEW\n",
    "    'total_deals',             # NEW\n",
    "    'customer_lifetime_days',  # NEW\n",
    "    'product_diversity', \n",
    "    'agent_count'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75ee4767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the new metrics\n",
    "account_metrics['win_rate'] = (account_metrics['total_wins'] /  account_metrics['total_opportunities'] * 100)\n",
    "\n",
    "account_metrics['loss_rate'] = (account_metrics['total_losses'] /  account_metrics['total_opportunities'] * 100)  # NEW\n",
    "\n",
    "account_metrics['deals_per_year'] = (account_metrics['total_deals'] / (account_metrics['customer_lifetime_days'] / 365 + 0.01))  # NEW (can't divide by zero)\n",
    "\n",
    "# Merge with accounts data\n",
    "merged_df = accounts.merge(account_metrics, on='account', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc909acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Days since last deal\n",
    "last_deal_dates = pipeline.groupby('account')['close_date'].max()\n",
    "merged_df['days_since_last_deal'] = (pd.Timestamp.now() - pd.to_datetime(last_deal_dates)).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e19ed1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nl/6y23jbvx6976ljxjh0vqgr7w0000gn/T/ipykernel_71888/1984320299.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['recent_revenue'].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Revenue momentum (recent performance vs historical)\n",
    "recent_cutoff = '2016-01-01'  # Guesstimate\n",
    "recent_revenue = (pipeline[pd.to_datetime(pipeline['close_date']) > recent_cutoff]\n",
    "                  .groupby('account')['close_value'].sum())\n",
    "merged_df = merged_df.merge(recent_revenue.rename('recent_revenue'), \n",
    "                            on='account', how='left')\n",
    "merged_df['recent_revenue'].fillna(0, inplace=True)\n",
    "merged_df['revenue_momentum'] = merged_df['recent_revenue'] / (merged_df['total_revenue'] + 1)  # +1 to avoid division by zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7682329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataframe shape: (85, 24)\n",
      "New features added: 24 total columns\n",
      "\n",
      "Column names:\n",
      "['account', 'sector', 'year_established', 'revenue', 'employees', 'office_location', 'subsidiary_of', 'total_opportunities', 'total_revenue', 'avg_deal_size', 'revenue_std', 'max_deal_size', 'total_wins', 'total_losses', 'total_deals', 'customer_lifetime_days', 'product_diversity', 'agent_count', 'win_rate', 'loss_rate', 'deals_per_year', 'days_since_last_deal', 'recent_revenue', 'revenue_momentum']\n",
      "\n",
      "First few rows:\n",
      "            account      sector  year_established  revenue  employees  \\\n",
      "0  acme_corporation  technology            1996.0  1100.04     2822.0   \n",
      "1        betasoloin     medical            1999.0   251.41      495.0   \n",
      "2          betatech     medical            1986.0   647.18     1185.0   \n",
      "3        bioholding     medical            2012.0   587.34     1356.0   \n",
      "4           bioplex     medical            1991.0   326.82     1016.0   \n",
      "\n",
      "  office_location     subsidiary_of  total_opportunities  total_revenue  \\\n",
      "0   united_states  acme_corporation                   68       106464.0   \n",
      "1   united_states  acme_corporation                   68       103644.0   \n",
      "2           kenya  acme_corporation                   92       110240.0   \n",
      "3     philippines  acme_corporation                   94        95239.0   \n",
      "4   united_states  acme_corporation                   53        70697.0   \n",
      "\n",
      "   avg_deal_size  ...  total_deals  customer_lifetime_days  product_diversity  \\\n",
      "0    1565.647059  ...           68                     377                  6   \n",
      "1    1524.176471  ...           68                     392                  7   \n",
      "2    1198.260870  ...           92                     383                  6   \n",
      "3    1013.180851  ...           94                     410                  6   \n",
      "4    1333.905660  ...           53                     388                  6   \n",
      "\n",
      "   agent_count  win_rate  loss_rate  deals_per_year  days_since_last_deal  \\\n",
      "0           16       0.0        0.0       65.204256                   NaN   \n",
      "1           19       0.0        0.0       62.732213                   NaN   \n",
      "2           13       0.0        0.0       86.848571                   NaN   \n",
      "3           13       0.0        0.0       82.944518                   NaN   \n",
      "4           14       0.0        0.0       49.393591                   NaN   \n",
      "\n",
      "   recent_revenue  revenue_momentum  \n",
      "0        106464.0          0.999991  \n",
      "1        103644.0          0.999990  \n",
      "2        110240.0          0.999991  \n",
      "3         95239.0          0.999990  \n",
      "4         70697.0          0.999986  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"Merged dataframe shape:\", merged_df.shape)\n",
    "print(f\"New features added: {merged_df.shape[1]} total columns\")\n",
    "print(\"\\nColumn names:\")\n",
    "print(merged_df.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1edea29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in new features:\n",
      "max_deal_size             0\n",
      "total_losses              0\n",
      "customer_lifetime_days    0\n",
      "loss_rate                 0\n",
      "deals_per_year            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMissing values in new features:\")\n",
    "print(merged_df[['max_deal_size', 'total_losses', 'customer_lifetime_days', \n",
    "                  'loss_rate', 'deals_per_year']].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c4bf05",
   "metadata": {},
   "source": [
    "## Step 4: Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8237ada5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to normalize everything to 0-100 scale\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1aef12e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target range: 2.19 to 65.00\n",
      "Target mean: 10.74\n",
      "Target distribution:\n",
      "count    85.000000\n",
      "mean     10.739276\n",
      "std       7.260205\n",
      "min       2.192426\n",
      "25%       7.633275\n",
      "50%       9.157848\n",
      "75%      12.071152\n",
      "max      65.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 100))\n",
    "\n",
    "# Components of health score\n",
    "components = pd.DataFrame()\n",
    "components['revenue_score'] = scaler.fit_transform(merged_df[['total_revenue']])[:, 0]\n",
    "components['win_rate_score'] = merged_df['win_rate']  # Already 0-100\n",
    "components['engagement_score'] = scaler.fit_transform(merged_df[['total_opportunities']])\n",
    "components['avg_deal_score'] = scaler.fit_transform(merged_df[['avg_deal_size']])\n",
    "\n",
    "# Weighted combination (ask JP and Fanizza if this makes sense)\n",
    "weights = {\n",
    "    'revenue_score': 0.5,      # 50% (most important)\n",
    "    'win_rate_score': 0.25,     # 25% (conversion matters)\n",
    "    'engagement_score': 0.15,   # 15% (activity level)\n",
    "    'avg_deal_score': 0.1      # 10% (deal quality)\n",
    "}\n",
    "\n",
    "y_target = (\n",
    "    components['revenue_score'] * weights['revenue_score'] +\n",
    "    components['win_rate_score'] * weights['win_rate_score'] +\n",
    "    components['engagement_score'] * weights['engagement_score'] +\n",
    "    components['avg_deal_score'] * weights['avg_deal_score']\n",
    ")\n",
    "\n",
    "# Verify it's 0-100\n",
    "print(f\"Target range: {y_target.min():.2f} to {y_target.max():.2f}\")\n",
    "print(f\"Target mean: {y_target.mean():.2f}\")\n",
    "print(f\"Target distribution:\\n{y_target.describe()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2666b7e7",
   "metadata": {},
   "source": [
    "## Preparing the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74d25fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (85, 29)\n",
      "Feature columns: ['revenue', 'employees', 'year_established', 'has_subsidiary', 'product_diversity', 'agent_count', 'sector_entertainment', 'sector_finance', 'sector_marketing', 'sector_medical', 'sector_retail', 'sector_services', 'sector_software', 'sector_technology', 'sector_telecommunications', 'office_location_brazil', 'office_location_china', 'office_location_germany', 'office_location_italy', 'office_location_japan', 'office_location_jordan', 'office_location_kenya', 'office_location_korea', 'office_location_norway', 'office_location_panama', 'office_location_philippines', 'office_location_poland', 'office_location_romania', 'office_location_united_states']\n"
     ]
    }
   ],
   "source": [
    "# Encode categorical variables\n",
    "merged_df['has_subsidiary'] = (merged_df['subsidiary_of'] != 'Independent').astype(int)\n",
    "\n",
    "# One-hot encode sector and location\n",
    "merged_df = pd.get_dummies(merged_df, columns=['sector', 'office_location'], drop_first=True)\n",
    "\n",
    "# Select features for model\n",
    "feature_cols = ['revenue', 'employees', 'year_established', 'has_subsidiary',\n",
    "                'product_diversity', 'agent_count'] + \\\n",
    "               [col for col in merged_df.columns if col.startswith('sector_') or \n",
    "                col.startswith('office_location_')]\n",
    "\n",
    "X = merged_df[feature_cols]\n",
    "y = y_target\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Feature columns: {X.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1248ea",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88fcd1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MODEL RESULTS\n",
      "                                 R2   MAE   RMSE\n",
      "Linear Regression_normalize  0.4064  4.60  10.28\n",
      "Linear Regression_clipping   0.4064  4.60  10.28\n",
      "Linear Regression_direct     0.4064  4.60  10.28\n",
      "Ridge_normalize              0.3908  4.51  10.42\n",
      "Ridge_clipping               0.3908  4.51  10.42\n",
      "Ridge_direct                 0.3908  4.51  10.42\n",
      "Random Forest_normalize      0.2973  4.55  11.19\n",
      "Random Forest_clipping       0.2862  4.58  11.28\n",
      "Random Forest_direct         0.2862  4.58  11.28\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Testing only the top 3 scaling methods to save time\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge': Ridge(alpha=1.0), # From JP's notes\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10)\n",
    "}\n",
    "\n",
    "scaling_methods = ['normalize', 'clipping', 'direct']  # Starting w/ just with these 3\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    for scaling in scaling_methods:\n",
    "        # Train model\n",
    "        if scaling == 'normalize':\n",
    "            y_train_scaled = y_train / 100\n",
    "            model.fit(X_train_scaled, y_train_scaled)\n",
    "            preds = model.predict(X_test_scaled) * 100\n",
    "        else:\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            preds = model.predict(X_test_scaled)\n",
    "        \n",
    "        # Apply scaling\n",
    "        if scaling == 'clipping':\n",
    "            preds = np.clip(preds, 0, 100)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        r2 = r2_score(y_test, preds) # Measures overall model fit\n",
    "        mae = mean_absolute_error(y_test, preds) # More of a general error measurement\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, preds)) # Penalizes large errors more than MAE\n",
    "        \n",
    "        results[f\"{model_name}_{scaling}\"] = {\n",
    "            'R2': round(r2, 4),\n",
    "            'MAE': round(mae, 2),\n",
    "            'RMSE': round(rmse, 2)\n",
    "        }\n",
    "\n",
    "# Show results\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"\\nMODEL RESULTS\")\n",
    "print(results_df.sort_values('R2', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee40b43",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c855a05",
   "metadata": {},
   "source": [
    "After evaluating, the best model is linear regression with any scaling method! It has the **highest R2 score** (explains the most variance) and the **lowest RMSE** (best at avoiding large prediction errors). It's also the **simplest model** and scaling didn't matter because all three scaling methods give identical results.\n",
    "\n",
    "**However, the R2 score is not > 0.7 as is required which means the model is only 38% accurate at predicting account health.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1cf096",
   "metadata": {},
   "source": [
    "Ridge performed slightly worse than linear regression which means there's no significant overfitting in the data (yay!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b7904e",
   "metadata": {},
   "source": [
    "Random forest was the worst which I think is because the dataset is too small for that model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b85942",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
