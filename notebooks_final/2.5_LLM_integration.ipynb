{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "464d2188",
   "metadata": {},
   "source": [
    "# LLM Integration Description\n",
    "LLM Integration refers to incorporation LLMs, in this case, Gemini, into businesses processes for enhanced efficiency, allowing for applications to leverage advanced NLP capabilites for a wide range of tasks.\n",
    "\n",
    "Some key concepts include...\n",
    "* API CAlls\n",
    "* Prompt Engineering\n",
    "* Data Handeling\n",
    "* RAG\n",
    "\n",
    "In this portion of the project, we are aiming to conduct LLM integration through: \n",
    "* Lead Scoring - identify and prioritize high value leads\n",
    "* Account Health - detects churn risks or upsell opportunities\n",
    "* Semantic Search - make chatbot retrieve and respond intelligently to business or sales data\n",
    "\n",
    "### Gemini: What is it?\n",
    "Gemini is an LLM developed by Google that allows for reasoning, code generation, and instruction following. \n",
    "\n",
    "### API - Application Programming Interface\n",
    "API will help us bridge between the LLM and the sent request. The API Key is a password that identifies the project when you use an APi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9261bdb7",
   "metadata": {},
   "source": [
    "## Step 1: Install Packages and Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbf8b2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install Gemini\n",
    "# pip install google-generativeai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dc3b1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to Gemini API\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables .env\n",
    "load_dotenv()\n",
    "\n",
    "# Get the Gemini API key from .env\n",
    "api_key = os.getenv('GEMINI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69ed107e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/google/api_core/_python_version_support.py:266: FutureWarning: You are using a Python version (3.10.10) which Google will stop supporting in new releases of google.api_core once it reaches its end of life (2026-10-04). Please upgrade to the latest Python version, or at least Python 3.11, to continue receiving updates for google.api_core past that date.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['models/embedding-gecko-001', 'models/gemini-2.5-pro-preview-03-25', 'models/gemini-2.5-flash', 'models/gemini-2.5-pro-preview-05-06', 'models/gemini-2.5-pro-preview-06-05', 'models/gemini-2.5-pro', 'models/gemini-2.0-flash-exp', 'models/gemini-2.0-flash', 'models/gemini-2.0-flash-001', 'models/gemini-2.0-flash-exp-image-generation', 'models/gemini-2.0-flash-lite-001', 'models/gemini-2.0-flash-lite', 'models/gemini-2.0-flash-lite-preview-02-05', 'models/gemini-2.0-flash-lite-preview', 'models/gemini-2.0-pro-exp', 'models/gemini-2.0-pro-exp-02-05', 'models/gemini-exp-1206', 'models/gemini-2.0-flash-thinking-exp-01-21', 'models/gemini-2.0-flash-thinking-exp', 'models/gemini-2.0-flash-thinking-exp-1219', 'models/gemini-2.5-flash-preview-tts', 'models/gemini-2.5-pro-preview-tts', 'models/learnlm-2.0-flash-experimental', 'models/gemma-3-1b-it', 'models/gemma-3-4b-it', 'models/gemma-3-12b-it', 'models/gemma-3-27b-it', 'models/gemma-3n-e4b-it', 'models/gemma-3n-e2b-it', 'models/gemini-flash-latest', 'models/gemini-flash-lite-latest', 'models/gemini-pro-latest', 'models/gemini-2.5-flash-lite', 'models/gemini-2.5-flash-image-preview', 'models/gemini-2.5-flash-image', 'models/gemini-2.5-flash-preview-09-2025', 'models/gemini-2.5-flash-lite-preview-09-2025', 'models/gemini-3-pro-preview', 'models/gemini-3-pro-image-preview', 'models/nano-banana-pro-preview', 'models/gemini-robotics-er-1.5-preview', 'models/gemini-2.5-computer-use-preview-10-2025', 'models/embedding-001', 'models/text-embedding-004', 'models/gemini-embedding-exp-03-07', 'models/gemini-embedding-exp', 'models/gemini-embedding-001', 'models/aqa', 'models/imagen-4.0-generate-preview-06-06', 'models/imagen-4.0-ultra-generate-preview-06-06', 'models/imagen-4.0-generate-001', 'models/imagen-4.0-ultra-generate-001', 'models/imagen-4.0-fast-generate-001', 'models/veo-2.0-generate-001', 'models/veo-3.0-generate-001', 'models/veo-3.0-fast-generate-001', 'models/veo-3.1-generate-preview', 'models/veo-3.1-fast-generate-preview', 'models/gemini-2.0-flash-live-001', 'models/gemini-live-2.5-flash-preview', 'models/gemini-2.5-flash-live-preview', 'models/gemini-2.5-flash-native-audio-latest', 'models/gemini-2.5-flash-native-audio-preview-09-2025']\n"
     ]
    }
   ],
   "source": [
    "#initialize Gemini and import necessary libraries\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import joblib\n",
    "\n",
    "# Configure Gemini with your API key\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "# Debug: Print available models\n",
    "print(\"Available models:\", [m.name for m in genai.list_models()])\n",
    "\n",
    "# Create the model with correct name\n",
    "model = genai.GenerativeModel('models/gemini-pro-latest')  # Update model name to match available models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd468b3e",
   "metadata": {},
   "source": [
    "## Step 2: Create Helper Funcitons for Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c32f367e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GBDTLeadScorer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        pipeline=None,\n",
    "        features: Optional[List[str]] = None,\n",
    "        model_path: Optional[str] = None,\n",
    "        feature_path: Optional[str] = None,\n",
    "    ):\n",
    "        if pipeline is not None:\n",
    "            self.pipeline = pipeline\n",
    "        elif model_path and os.path.exists(model_path):\n",
    "            self.pipeline = joblib.load(model_path)\n",
    "        else:\n",
    "            raise ValueError(\"Provide a fitted pipeline or a valid model_path.\")\n",
    "\n",
    "        # Prefer explicit features arg; else try load from file; else None\n",
    "        if features is not None:\n",
    "            self.features = features\n",
    "        elif feature_path and os.path.exists(feature_path):\n",
    "            self.features = joblib.load(feature_path)\n",
    "        else:\n",
    "            self.features = None  # pipeline must handle missing order\n",
    "\n",
    "    def predict(self, features: Dict[str, Any]) -> float:\n",
    "        X = pd.DataFrame([features])\n",
    "        if self.features:\n",
    "            X = X.reindex(columns=self.features)\n",
    "        proba = self.pipeline.predict_proba(X)[:, 1]\n",
    "        return float(proba[0])\n",
    "\n",
    "\n",
    "class AccountHealthScorer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        pipeline=None,\n",
    "        features: Optional[List[str]] = None,\n",
    "        model_path: Optional[str] = None,\n",
    "        feature_path: Optional[str] = None,\n",
    "    ):\n",
    "        if pipeline is not None:\n",
    "            self.pipeline = pipeline\n",
    "        elif model_path and os.path.exists(model_path):\n",
    "            self.pipeline = joblib.load(model_path)\n",
    "        else:\n",
    "            raise ValueError(\"Provide a fitted pipeline or a valid model_path.\")\n",
    "\n",
    "        if features is not None:\n",
    "            self.features = features\n",
    "        elif feature_path and os.path.exists(feature_path):\n",
    "            self.features = joblib.load(feature_path)\n",
    "        else:\n",
    "            self.features = None\n",
    "\n",
    "    def predict(self, features: Dict[str, Any]) -> float:\n",
    "        X = pd.DataFrame([features])\n",
    "        if self.features:\n",
    "            X = X.reindex(columns=self.features)\n",
    "        proba = self.pipeline.predict_proba(X)[:, 1]  # adjust if needed\n",
    "        return float(proba[0])\n",
    "\n",
    "\n",
    "class SemanticSearcher:\n",
    "    \"\"\"Use an existing Chroma collection/client if you already created one.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        collection=None,\n",
    "        client=None,\n",
    "        persist_dir: Optional[str] = None,\n",
    "        collection_name: str = \"crm_docs\",\n",
    "    ):\n",
    "        if collection is not None:\n",
    "            self.collection = collection\n",
    "        else:\n",
    "            if client is None:\n",
    "                if not persist_dir:\n",
    "                    raise ValueError(\"Provide collection/client OR persist_dir.\")\n",
    "                import chromadb\n",
    "                client = chromadb.PersistentClient(path=persist_dir)\n",
    "            self.collection = client.get_or_create_collection(collection_name)\n",
    "\n",
    "    def search(self, query: str, n_results: int = 5, where: Optional[Dict[str, Any]] = None):\n",
    "        res = self.collection.query(query_texts=[query], n_results=n_results, where=where)\n",
    "        docs = res.get(\"documents\", [[]])[0]\n",
    "        metas = res.get(\"metadatas\", [[]])[0]\n",
    "        dists = res.get(\"distances\", [[]])[0] if \"distances\" in res else [None]*len(docs)\n",
    "        out = []\n",
    "        for i, txt in enumerate(docs):\n",
    "            meta = metas[i] if i < len(metas) else {}\n",
    "            out.append({\n",
    "                \"text\": txt,\n",
    "                \"source\": (meta or {}).get(\"source\", f\"doc_{i}\"),\n",
    "                \"score\": dists[i]\n",
    "            })\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f560755f",
   "metadata": {},
   "source": [
    "## Step 3: Create Main Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f639ec6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"GEMINI_API_KEY not set in your .env\")\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "class CRMAssistant:\n",
    "    def __init__(self, lead_scorer, health_scorer, semantic_searcher, model_name=\"gemini-pro\"):\n",
    "        self.lead_scorer = lead_scorer\n",
    "        self.health_scorer = health_scorer\n",
    "        self.semantic_searcher = semantic_searcher\n",
    "        self.model = genai.GenerativeModel(model_name)\n",
    "\n",
    "    @staticmethod\n",
    "    def _fmt(docs: List[Dict[str, Any]]) -> str:\n",
    "        return \"\\n\".join(f\"[{d.get('source','doc')}] {d.get('text','')}\" for d in docs)\n",
    "\n",
    "    def process_lead_score(self, lead_score: float, lead_data: Dict[str, Any]) -> str:\n",
    "        prompt = f\"\"\"\n",
    "You are a CRM assistant. Given a lead score of {lead_score:.2f} and this lead data:\n",
    "{lead_data}\n",
    "\n",
    "Provide:\n",
    "- 2–4 next actions,\n",
    "- a short rationale referencing top drivers,\n",
    "- a one-line priority (High/Med/Low).\n",
    "\"\"\"\n",
    "        return self.model.generate_content(prompt).text\n",
    "\n",
    "    def analyze_account_health(self, health_score: float, account_data: Dict[str, Any]) -> str:\n",
    "        prompt = f\"\"\"\n",
    "You are a CRM assistant. Account health score: {health_score:.2f}.\n",
    "Account data:\n",
    "{account_data}\n",
    "\n",
    "Return:\n",
    "- 3 targeted recommendations,\n",
    "- key risks,\n",
    "- owner + due date for the first action.\n",
    "\"\"\"\n",
    "        return self.model.generate_content(prompt).text\n",
    "\n",
    "    def semantic_search_response(self, query: str, context_docs: List[Dict[str, Any]]) -> str:\n",
    "        ctx = self._fmt(context_docs)\n",
    "        prompt = f\"\"\"\n",
    "Use the CRM context to answer. If info is missing, say what is needed.\n",
    "\n",
    "Context:\n",
    "{ctx}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Reply with a brief summary and bullet points. Cite sources in [brackets].\n",
    "\"\"\"\n",
    "        return self.model.generate_content(prompt).text\n",
    "\n",
    "    def process_query(self, query: str, context: Optional[Dict[str, Any]] = None) -> str:\n",
    "        q = query.lower()\n",
    "        if \"lead\" in q and any(k in q for k in [\"score\", \"convert\", \"probability\"]):\n",
    "            if context is None:\n",
    "                return \"I need lead features (or a lead_id I can fetch) to score this lead.\"\n",
    "            score = self.lead_scorer.predict(context)\n",
    "            return self.process_lead_score(score, context)\n",
    "\n",
    "        if any(k in q for k in [\"health\", \"churn\", \"risk\", \"renewal\"]):\n",
    "            if context is None:\n",
    "                return \"I need account features (or an account_id I can fetch) to assess health.\"\n",
    "            score = self.health_scorer.predict(context)\n",
    "            return self.analyze_account_health(score, context)\n",
    "\n",
    "        docs = self.semantic_searcher.search(query)\n",
    "        return self.semantic_search_response(query, docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0759ddb4",
   "metadata": {},
   "source": [
    "## Step 4: Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b03caa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing Gemini LLM Integration ===\n",
      "\n",
      "Prompt: What are the key benefits of using LLMs in CRM systems?\n",
      "\n",
      "Response: Of course. Integrating Large Language Models (LLMs) into Customer Relationship Management (CRM) systems is a transformative shift, turning CRMs from passive systems of record into proactive, intelligent partners.\n",
      "\n",
      "The key benefits can be grouped into four main areas: **Supercharging Sales Teams**, **Revolutionizing Customer Service**, **Enhancing Marketing Personalization**, and **Boosting Overall Operational Efficiency**.\n",
      "\n",
      "Here’s a detailed breakdown of the key benefits in each area:\n",
      "\n",
      "---\n",
      "\n",
      "### 1. Supercharging Sales Teams\n",
      "\n",
      "LLMs act as a \"co-pilot\" for sales representatives, automating tedious tasks and providing intelligent guidance to help them close deals faster.\n",
      "\n",
      "*   **Automated Communication & Content Generation:**\n",
      "    *   **Benefit:** Dramatically reduces the time spent on writing. Sales reps can instantly generate personalized outreach emails, follow-up messages, and meeting preparation notes based on a customer's profile, industry, and past interactions.\n",
      "    *   **Example:** A sales rep clicks a button on a lead's profile, and the LLM drafts a compelling introductory email that references the lead's recent activity on LinkedIn and mentions a relevant case study.\n",
      "\n",
      "*   **Intelligent Summarization:**\n",
      "    *   **Benefit:** Saves hours of reading and helps reps get up to speed instantly. LLMs can summarize long email threads, call transcripts, and meeting notes into concise, actionable bullet points.\n",
      "    *   **Example:** A new account executive takes over a deal. Instead of reading dozens of emails, they get an LLM-generated summary: \"Customer is concerned about integration with SAP, is price-sensitive, and the key decision-maker is Jane Doe. Next step is to schedule a technical demo.\"\n",
      "\n",
      "*   **Real-time Sales Coaching and Guidance:**\n",
      "    *   **Benefit:** Improves sales effectiveness and consistency. During a live call or while drafting an email, the LLM can analyze the conversation and suggest talking points, answers to objections, or relevant product information in real-time.\n",
      "    *   **Example:** A customer says, \"Your competitor is 20% cheaper.\" The LLM instantly surfaces a battle card highlighting key differentiators and value propositions to counter the objection.\n",
      "\n",
      "*   **Enhanced Lead Scoring and Opportunity Insights:**\n",
      "    *   **Benefit:** Focuses sales efforts on the most promising deals. By analyzing the sentiment and content of customer communications, LLMs can identify buying signals (e.g., use of words like \"budget,\" \"timeline,\" \"proposal\") or risks (e.g., mentions of competitors, frustration) that traditional scoring misses.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Revolutionizing Customer Service\n",
      "\n",
      "LLMs empower support agents to resolve issues faster and provide a more empathetic, human-like experience for customers.\n",
      "\n",
      "*   **Advanced, Human-like Chatbots & Virtual Agents:**\n",
      "    *   **Benefit:** Provides instant, 24/7 support that can handle complex, multi-turn conversations, unlike rigid, rule-based bots. This frees up human agents for the most critical issues.\n",
      "    *   **Example:** A customer can ask a chatbot, \"I ordered a blue shirt last week but it hasn't arrived, and I think I used the wrong address.\" The LLM-powered bot can understand the entire context, look up the order, identify the address issue, and guide the customer through correcting it.\n",
      "\n",
      "*   **Agent Assist & Co-pilot:**\n",
      "    *   **Benefit:** Increases agent productivity and reduces response times. While an agent is talking to a customer, the LLM can listen in, automatically find relevant knowledge base articles, and draft accurate, empathetic responses for the agent to review and send.\n",
      "    *   **Example:** As a customer describes a technical problem, the LLM surfaces the exact troubleshooting guide and pre-writes a step-by-step response, cutting the agent's research and typing time by over 50%.\n",
      "\n",
      "*   **Automated Case Summarization and Categorization:**\n",
      "    *   **Benefit:** Ensures faster routing and resolution. When a support ticket arrives, the LLM instantly reads the customer's message, summarizes the core issue, and automatically categorizes and routes it to the correct department (e.g., Billing, Technical Support, Returns).\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Enhancing Marketing Personalization\n",
      "\n",
      "LLMs allow marketers to move beyond basic segmentation and create truly one-to-one marketing campaigns at scale.\n",
      "\n",
      "*   **Hyper-Personalized Campaign Content:**\n",
      "    *   **Benefit:** Boosts engagement and conversion rates. Marketers can use LLMs to generate thousands of variations of ad copy, email subject lines, and body content, each tailored to a specific customer segment's interests, past purchases, and browsing history.\n",
      "    *   **Example:** An e-commerce company can generate an email campaign where each customer sees a unique product description that highlights the features most relevant to them, based on their previous interactions.\n",
      "\n",
      "*   **Deeper Sentiment Analysis & Voice of the Customer (VoC):**\n",
      "    *   **Benefit:** Provides a real-time pulse on customer perception. LLMs can analyze thousands of customer reviews, social media comments, and survey responses to identify emerging trends, product issues, or positive feedback with a level of nuance that keyword-based tools can't match.\n",
      "\n",
      "*   **Intelligent Audience Segmentation:**\n",
      "    *   **Benefit:** Enables highly specific targeting without complex queries. Marketers can use natural language to define new audience segments.\n",
      "    *   **Example:** A marketer could simply ask the CRM, \"Create a list of all customers who have purchased product X, haven't logged in for 90 days, and have mentioned 'poor user interface' in a support ticket.\"\n",
      "\n",
      "---\n",
      "\n",
      "### 4. Boosting Overall Operational Efficiency & Data Analytics\n",
      "\n",
      "LLMs make the CRM itself easier to use and unlock the value hidden within its data for everyone, not just analysts.\n",
      "\n",
      "*   **Natural Language Data Querying:**\n",
      "    *   **Benefit:** Democratizes data access. Any user, regardless of technical skill, can ask complex questions about their data in plain English and get instant reports, charts, and answers.\n",
      "    *   **Example:** A sales manager can ask, \"What was our total revenue in the West region last quarter, and how does that compare to the year before?\" instead of needing to build a custom report.\n",
      "\n",
      "*   **Automated Data Entry and Cleansing:**\n",
      "    *   **Benefit:** Improves data quality and saves administrative time. LLMs can extract structured information from unstructured text, such as automatically creating a new contact record from an email signature or updating an opportunity stage based on the content of a sales call summary.\n",
      "\n",
      "*   **Predictive Analytics and Forecasting:**\n",
      "    *   **Benefit:** Provides more accurate and data-driven business planning. By analyzing historical trends and the text within deal notes, LLMs can help generate more reliable sales forecasts and identify at-risk customers before they churn.\n",
      "\n",
      "### Summary Table: Key Benefits at a Glance\n",
      "\n",
      "| Category | Specific Benefit | Impact |\n",
      "| :--- | :--- | :--- |\n",
      "| **Sales** | Automated Content Generation | Saves time, increases outreach volume and quality. |\n",
      "| | Intelligent Summarization | Faster onboarding, quicker deal handoffs. |\n",
      "| | Real-time Coaching | Improves win rates, more effective reps. |\n",
      "| **Customer Service** | Advanced Chatbots | 24/7 support, lower operational costs. |\n",
      "| | Agent Assist (Co-pilot) | Faster resolution times (FCR), higher agent satisfaction. |\n",
      "| | Automated Case Routing | Reduces manual work, quicker response to customers. |\n",
      "| **Marketing** | Hyper-Personalization | Higher engagement, better conversion rates. |\n",
      "| | Sentiment Analysis | Proactive issue resolution, deeper customer understanding. |\n",
      "| | Intelligent Segmentation | More effective and targeted campaigns. |\n",
      "| **Operations** | Natural Language Querying | Empowers non-technical users to access data insights. |\n",
      "| | Automated Data Entry | Better data hygiene, less administrative overhead. |\n",
      "| | Predictive Forecasting | More accurate business planning and resource allocation. |\n",
      "\n",
      "In conclusion, the integration of LLMs fundamentally changes the value proposition of a CRM. It evolves from a database for storing customer information into an intelligent engine that actively helps you build better relationships, close more deals, and operate more efficiently.\n",
      "\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "# Basic LLM Integration Test\n",
    "# Test function remains the same\n",
    "def test_llm_integration():\n",
    "    prompt = \"What are the key benefits of using LLMs in CRM systems?\"\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config={\n",
    "                \"temperature\": 0.7,\n",
    "                \"top_p\": 0.8,\n",
    "                \"top_k\": 40\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        print(\"\\n=== Testing Gemini LLM Integration ===\")\n",
    "        print(\"\\nPrompt:\", prompt)\n",
    "        print(\"\\nResponse:\", response.text)\n",
    "        print(\"\\n=====================================\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error testing LLM integration: {str(e)}\")\n",
    "        print(\"API Key configured:\", bool(api_key))\n",
    "        print(\"Available models:\", [m.name for m in genai.list_models()])\n",
    "\n",
    "# Run the test\n",
    "test_llm_integration()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
