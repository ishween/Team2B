{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "464d2188",
   "metadata": {},
   "source": [
    "# LLM Integration Description\n",
    "LLM Integration refers to incorporation LLMs, in this case, Gemini, into businesses processes for enhanced efficiency, allowing for applications to leverage advanced NLP capabilites for a wide range of tasks.\n",
    "\n",
    "Some key concepts include...\n",
    "* API CAlls\n",
    "* Prompt Engineering\n",
    "* Data Handeling\n",
    "* RAG\n",
    "\n",
    "In this portion of the project, we are aiming to conduct LLM integration through: \n",
    "* Lead Scoring - identify and prioritize high value leads\n",
    "* Account Health - detects churn risks or upsell opportunities\n",
    "* Semantic Search - make chatbot retrieve and respond intelligently to business or sales data\n",
    "\n",
    "### Gemini: What is it?\n",
    "Gemini is an LLM developed by Google that allows for reasoning, code generation, and instruction following. \n",
    "\n",
    "### API - Application Programming Interface\n",
    "API will help us bridge between the LLM and the sent request. The API Key is a password that identifies the project when you use an APi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9261bdb7",
   "metadata": {},
   "source": [
    "## Step 1: Install Packages and Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbf8b2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install Gemini\n",
    "# pip install google-generativeai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dc3b1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to Gemini API\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables .env\n",
    "load_dotenv()\n",
    "\n",
    "# Get the Gemini API key from .env\n",
    "api_key = os.getenv('GEMINI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69ed107e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: module 'importlib.metadata' has no attribute 'packages_distributions'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nissiotoo/Library/Python/3.9/lib/python/site-packages/google/api_core/_python_version_support.py:252: FutureWarning: You are using a Python version (3.9.6) past its end of life. Google will update google.api_core with critical bug fixes on a best-effort basis, but not with any other fixes or features. Please upgrade to the latest Python version, or at least Python 3.10, and then update google.api_core.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/Users/nissiotoo/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/nissiotoo/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['models/embedding-gecko-001', 'models/gemini-2.5-pro-preview-03-25', 'models/gemini-2.5-flash-preview-05-20', 'models/gemini-2.5-flash', 'models/gemini-2.5-flash-lite-preview-06-17', 'models/gemini-2.5-pro-preview-05-06', 'models/gemini-2.5-pro-preview-06-05', 'models/gemini-2.5-pro', 'models/gemini-2.0-flash-exp', 'models/gemini-2.0-flash', 'models/gemini-2.0-flash-001', 'models/gemini-2.0-flash-exp-image-generation', 'models/gemini-2.0-flash-lite-001', 'models/gemini-2.0-flash-lite', 'models/gemini-2.0-flash-preview-image-generation', 'models/gemini-2.0-flash-lite-preview-02-05', 'models/gemini-2.0-flash-lite-preview', 'models/gemini-2.0-pro-exp', 'models/gemini-2.0-pro-exp-02-05', 'models/gemini-exp-1206', 'models/gemini-2.0-flash-thinking-exp-01-21', 'models/gemini-2.0-flash-thinking-exp', 'models/gemini-2.0-flash-thinking-exp-1219', 'models/gemini-2.5-flash-preview-tts', 'models/gemini-2.5-pro-preview-tts', 'models/learnlm-2.0-flash-experimental', 'models/gemma-3-1b-it', 'models/gemma-3-4b-it', 'models/gemma-3-12b-it', 'models/gemma-3-27b-it', 'models/gemma-3n-e4b-it', 'models/gemma-3n-e2b-it', 'models/gemini-flash-latest', 'models/gemini-flash-lite-latest', 'models/gemini-pro-latest', 'models/gemini-2.5-flash-lite', 'models/gemini-2.5-flash-image-preview', 'models/gemini-2.5-flash-image', 'models/gemini-2.5-flash-preview-09-2025', 'models/gemini-2.5-flash-lite-preview-09-2025', 'models/gemini-robotics-er-1.5-preview', 'models/gemini-2.5-computer-use-preview-10-2025', 'models/embedding-001', 'models/text-embedding-004', 'models/gemini-embedding-exp-03-07', 'models/gemini-embedding-exp', 'models/gemini-embedding-001', 'models/aqa', 'models/imagen-3.0-generate-002', 'models/imagen-4.0-generate-preview-06-06', 'models/imagen-4.0-ultra-generate-preview-06-06', 'models/imagen-4.0-generate-001', 'models/imagen-4.0-ultra-generate-001', 'models/imagen-4.0-fast-generate-001', 'models/veo-2.0-generate-001', 'models/veo-3.0-generate-preview', 'models/veo-3.0-fast-generate-preview', 'models/veo-3.0-generate-001', 'models/veo-3.0-fast-generate-001', 'models/veo-3.1-generate-preview', 'models/veo-3.1-fast-generate-preview', 'models/gemini-2.0-flash-live-001', 'models/gemini-live-2.5-flash-preview', 'models/gemini-2.5-flash-live-preview', 'models/gemini-2.5-flash-native-audio-latest', 'models/gemini-2.5-flash-native-audio-preview-09-2025']\n"
     ]
    }
   ],
   "source": [
    "#initialize Gemini and import necessary libraries\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import joblib\n",
    "\n",
    "# Configure Gemini with your API key\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "# Debug: Print available models\n",
    "print(\"Available models:\", [m.name for m in genai.list_models()])\n",
    "\n",
    "# Create the model with correct name\n",
    "model = genai.GenerativeModel('models/gemini-pro-latest')  # Update model name to match available models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd468b3e",
   "metadata": {},
   "source": [
    "## Step 2: Create Helper Funcitons for Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c32f367e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GBDTLeadScorer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        pipeline=None,\n",
    "        features: Optional[List[str]] = None,\n",
    "        model_path: Optional[str] = None,\n",
    "        feature_path: Optional[str] = None,\n",
    "    ):\n",
    "        if pipeline is not None:\n",
    "            self.pipeline = pipeline\n",
    "        elif model_path and os.path.exists(model_path):\n",
    "            self.pipeline = joblib.load(model_path)\n",
    "        else:\n",
    "            raise ValueError(\"Provide a fitted pipeline or a valid model_path.\")\n",
    "\n",
    "        # Prefer explicit features arg; else try load from file; else None\n",
    "        if features is not None:\n",
    "            self.features = features\n",
    "        elif feature_path and os.path.exists(feature_path):\n",
    "            self.features = joblib.load(feature_path)\n",
    "        else:\n",
    "            self.features = None  # pipeline must handle missing order\n",
    "\n",
    "    def predict(self, features: Dict[str, Any]) -> float:\n",
    "        X = pd.DataFrame([features])\n",
    "        if self.features:\n",
    "            X = X.reindex(columns=self.features)\n",
    "        proba = self.pipeline.predict_proba(X)[:, 1]\n",
    "        return float(proba[0])\n",
    "\n",
    "\n",
    "class AccountHealthScorer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        pipeline=None,\n",
    "        features: Optional[List[str]] = None,\n",
    "        model_path: Optional[str] = None,\n",
    "        feature_path: Optional[str] = None,\n",
    "    ):\n",
    "        if pipeline is not None:\n",
    "            self.pipeline = pipeline\n",
    "        elif model_path and os.path.exists(model_path):\n",
    "            self.pipeline = joblib.load(model_path)\n",
    "        else:\n",
    "            raise ValueError(\"Provide a fitted pipeline or a valid model_path.\")\n",
    "\n",
    "        if features is not None:\n",
    "            self.features = features\n",
    "        elif feature_path and os.path.exists(feature_path):\n",
    "            self.features = joblib.load(feature_path)\n",
    "        else:\n",
    "            self.features = None\n",
    "\n",
    "    def predict(self, features: Dict[str, Any]) -> float:\n",
    "        X = pd.DataFrame([features])\n",
    "        if self.features:\n",
    "            X = X.reindex(columns=self.features)\n",
    "        proba = self.pipeline.predict_proba(X)[:, 1]  # adjust if needed\n",
    "        return float(proba[0])\n",
    "\n",
    "\n",
    "class SemanticSearcher:\n",
    "    \"\"\"Use an existing Chroma collection/client if you already created one.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        collection=None,\n",
    "        client=None,\n",
    "        persist_dir: Optional[str] = None,\n",
    "        collection_name: str = \"crm_docs\",\n",
    "    ):\n",
    "        if collection is not None:\n",
    "            self.collection = collection\n",
    "        else:\n",
    "            if client is None:\n",
    "                if not persist_dir:\n",
    "                    raise ValueError(\"Provide collection/client OR persist_dir.\")\n",
    "                import chromadb\n",
    "                client = chromadb.PersistentClient(path=persist_dir)\n",
    "            self.collection = client.get_or_create_collection(collection_name)\n",
    "\n",
    "    def search(self, query: str, n_results: int = 5, where: Optional[Dict[str, Any]] = None):\n",
    "        res = self.collection.query(query_texts=[query], n_results=n_results, where=where)\n",
    "        docs = res.get(\"documents\", [[]])[0]\n",
    "        metas = res.get(\"metadatas\", [[]])[0]\n",
    "        dists = res.get(\"distances\", [[]])[0] if \"distances\" in res else [None]*len(docs)\n",
    "        out = []\n",
    "        for i, txt in enumerate(docs):\n",
    "            meta = metas[i] if i < len(metas) else {}\n",
    "            out.append({\n",
    "                \"text\": txt,\n",
    "                \"source\": (meta or {}).get(\"source\", f\"doc_{i}\"),\n",
    "                \"score\": dists[i]\n",
    "            })\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f560755f",
   "metadata": {},
   "source": [
    "## Step 3: Create Main Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f639ec6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"GEMINI_API_KEY not set in your .env\")\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "class CRMAssistant:\n",
    "    def __init__(self, lead_scorer, health_scorer, semantic_searcher, model_name=\"gemini-pro\"):\n",
    "        self.lead_scorer = lead_scorer\n",
    "        self.health_scorer = health_scorer\n",
    "        self.semantic_searcher = semantic_searcher\n",
    "        self.model = genai.GenerativeModel(model_name)\n",
    "\n",
    "    @staticmethod\n",
    "    def _fmt(docs: List[Dict[str, Any]]) -> str:\n",
    "        return \"\\n\".join(f\"[{d.get('source','doc')}] {d.get('text','')}\" for d in docs)\n",
    "\n",
    "    def process_lead_score(self, lead_score: float, lead_data: Dict[str, Any]) -> str:\n",
    "        prompt = f\"\"\"\n",
    "You are a CRM assistant. Given a lead score of {lead_score:.2f} and this lead data:\n",
    "{lead_data}\n",
    "\n",
    "Provide:\n",
    "- 2â€“4 next actions,\n",
    "- a short rationale referencing top drivers,\n",
    "- a one-line priority (High/Med/Low).\n",
    "\"\"\"\n",
    "        return self.model.generate_content(prompt).text\n",
    "\n",
    "    def analyze_account_health(self, health_score: float, account_data: Dict[str, Any]) -> str:\n",
    "        prompt = f\"\"\"\n",
    "You are a CRM assistant. Account health score: {health_score:.2f}.\n",
    "Account data:\n",
    "{account_data}\n",
    "\n",
    "Return:\n",
    "- 3 targeted recommendations,\n",
    "- key risks,\n",
    "- owner + due date for the first action.\n",
    "\"\"\"\n",
    "        return self.model.generate_content(prompt).text\n",
    "\n",
    "    def semantic_search_response(self, query: str, context_docs: List[Dict[str, Any]]) -> str:\n",
    "        ctx = self._fmt(context_docs)\n",
    "        prompt = f\"\"\"\n",
    "Use the CRM context to answer. If info is missing, say what is needed.\n",
    "\n",
    "Context:\n",
    "{ctx}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Reply with a brief summary and bullet points. Cite sources in [brackets].\n",
    "\"\"\"\n",
    "        return self.model.generate_content(prompt).text\n",
    "\n",
    "    def process_query(self, query: str, context: Optional[Dict[str, Any]] = None) -> str:\n",
    "        q = query.lower()\n",
    "        if \"lead\" in q and any(k in q for k in [\"score\", \"convert\", \"probability\"]):\n",
    "            if context is None:\n",
    "                return \"I need lead features (or a lead_id I can fetch) to score this lead.\"\n",
    "            score = self.lead_scorer.predict(context)\n",
    "            return self.process_lead_score(score, context)\n",
    "\n",
    "        if any(k in q for k in [\"health\", \"churn\", \"risk\", \"renewal\"]):\n",
    "            if context is None:\n",
    "                return \"I need account features (or an account_id I can fetch) to assess health.\"\n",
    "            score = self.health_scorer.predict(context)\n",
    "            return self.analyze_account_health(score, context)\n",
    "\n",
    "        docs = self.semantic_searcher.search(query)\n",
    "        return self.semantic_search_response(query, docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0759ddb4",
   "metadata": {},
   "source": [
    "## Step 4: Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b03caa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing Gemini LLM Integration ===\n",
      "\n",
      "Prompt: What are the key benefits of using LLMs in CRM systems?\n",
      "\n",
      "Response: Of course. The integration of Large Language Models (LLMs) into Customer Relationship Management (CRM) systems is a game-changer, transforming them from passive databases into proactive, intelligent partners.\n",
      "\n",
      "The key benefits can be broken down by business function:\n",
      "\n",
      "---\n",
      "\n",
      "### 1. Enhanced Sales Productivity and Effectiveness\n",
      "\n",
      "Sales teams spend a significant amount of time on administrative tasks rather than selling. LLMs directly address this inefficiency.\n",
      "\n",
      "*   **Automated Communication Drafting:** LLMs can instantly generate personalized outreach emails, follow-ups, and proposals based on customer data in the CRM.\n",
      "    *   **Benefit:** Massively reduces the time reps spend writing, allowing them to engage more leads. It also ensures a consistent and high-quality tone across the team.\n",
      "\n",
      "*   **Intelligent Call & Meeting Summarization:** LLMs can transcribe audio from sales calls, summarize the key points, identify customer sentiment, and extract action items directly into the CRM.\n",
      "    *   **Benefit:** Eliminates manual note-taking, creates perfect records for future reference, and makes coaching and deal reviews far more efficient.\n",
      "\n",
      "*   **\"Deal Gaps\" and Next-Best-Action Suggestions:** By analyzing the entire communication history of a deal (emails, calls, notes), an LLM can identify potential risks, suggest the next logical step (e.g., \"You haven't discussed pricing with the decision-maker yet\"), or recommend relevant content to share.\n",
      "    *   **Benefit:** Acts as a co-pilot for sales reps, improving forecasting accuracy and increasing the probability of closing deals.\n",
      "\n",
      "*   **Lead Prioritization and Scoring:** LLMs can analyze the content of a lead's inquiries and interactions to understand their intent and urgency, providing a more nuanced lead score than traditional metric-based systems.\n",
      "    *   **Benefit:** Helps sales teams focus their energy on the most promising leads, improving conversion rates.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Revolutionized Customer Service and Support\n",
      "\n",
      "LLMs empower support teams to provide faster, more accurate, and more empathetic service.\n",
      "\n",
      "*   **Smarter Chatbots and Virtual Agents:** LLM-powered chatbots can understand complex, multi-turn conversations and provide nuanced answers by drawing from a knowledge base. They can handle a much wider range of queries than traditional, rule-based bots.\n",
      "    *   **Benefit:** Provides 24/7 instant support, resolves a higher percentage of issues without human intervention, and dramatically reduces operational costs.\n",
      "\n",
      "*   **Agent Assist and Suggested Replies:** When a human agent takes over, the LLM can listen in or read the ticket and provide real-time suggestions, relevant knowledge base articles, and pre-drafted, context-aware replies.\n",
      "    *   **Benefit:** Drastically reduces response times, improves first-contact resolution rates, and helps new agents get up to speed quickly.\n",
      "\n",
      "*   **Automated Ticket Summarization and Routing:** LLMs can read an incoming support ticket, understand its intent, sentiment, and urgency, summarize it for the agent, and automatically route it to the correct department (e.g., Billing, Technical Support).\n",
      "    *   **Benefit:** Ensures tickets get to the right person faster, leading to quicker resolutions and a better customer experience.\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Supercharged Marketing and Personalization\n",
      "\n",
      "Marketers can use LLMs to create more relevant and effective campaigns at scale.\n",
      "\n",
      "*   **Hyper-Personalized Content Generation:** LLMs can generate marketing copy (emails, social media posts, ad copy) tailored to specific customer segments or even individuals based on their purchase history, support interactions, and browsing behavior stored in the CRM.\n",
      "    *   **Benefit:** Increases engagement, click-through rates, and conversion rates by delivering messages that truly resonate with the audience.\n",
      "\n",
      "*   **Advanced Audience Segmentation:** By analyzing unstructured data like support ticket text and survey responses, LLMs can help identify nuanced customer segments that would be missed by traditional filtering (e.g., \"customers who are happy with the product but frustrated with the delivery process\").\n",
      "    *   **Benefit:** Enables highly targeted and more effective marketing campaigns.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. Streamlined Operations and Data Management\n",
      "\n",
      "The foundation of a good CRM is good data. LLMs help ensure data is clean, complete, and easy to access.\n",
      "\n",
      "*   **Automated Data Entry and Enrichment:** LLMs can automatically extract information from emails (like a new contact in a signature) or web pages and populate or update records in the CRM, eliminating manual data entry.\n",
      "    *   **Benefit:** Saves countless hours of administrative work, reduces human error, and improves overall data hygiene.\n",
      "\n",
      "*   **Natural Language Search and Reporting:** Users can ask the CRM questions in plain English, like \"Show me all deals over $50k in the pipeline for Q4 in the manufacturing sector\" or \"Summarize our top customer complaints from last month.\"\n",
      "    *   **Benefit:** Democratizes data access. Anyone can get complex insights without needing to know how to build a custom report, leading to faster, data-driven decisions.\n",
      "\n",
      "### Summary: The Core Shift\n",
      "\n",
      "The overarching benefit is the transformation of the CRM from a **System of Record** to a **System of Intelligence**.\n",
      "\n",
      "*   **Before LLMs:** A CRM was a place to *store* information. Its value depended on how diligently humans entered and interpreted the data.\n",
      "*   **With LLMs:** The CRM becomes a proactive partner that *understands* the information, *automates* workflows, and *generates insights* to help every employee be more effective in their role. This leads to increased revenue, lower operational costs, and a vastly improved customer experience.\n",
      "\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "# Basic LLM Integration Test\n",
    "# Test function remains the same\n",
    "def test_llm_integration():\n",
    "    prompt = \"What are the key benefits of using LLMs in CRM systems?\"\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config={\n",
    "                \"temperature\": 0.7,\n",
    "                \"top_p\": 0.8,\n",
    "                \"top_k\": 40\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        print(\"\\n=== Testing Gemini LLM Integration ===\")\n",
    "        print(\"\\nPrompt:\", prompt)\n",
    "        print(\"\\nResponse:\", response.text)\n",
    "        print(\"\\n=====================================\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error testing LLM integration: {str(e)}\")\n",
    "        print(\"API Key configured:\", bool(api_key))\n",
    "        print(\"Available models:\", [m.name for m in genai.list_models()])\n",
    "\n",
    "# Run the test\n",
    "test_llm_integration()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
