{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91c950da",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da666554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nissiotoo/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/nissiotoo/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5117767",
   "metadata": {},
   "source": [
    "# GenericRecord Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98b8bcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GenericRecord:\n",
    "    data: Dict[str, Any]\n",
    "    primary_key: str\n",
    "\n",
    "    def to_text(self, text_columns: Optional[List[str]] = None) -> str:\n",
    "        if text_columns:\n",
    "            relevant_data = {k: v for k, v in self.data.items() if k in text_columns}\n",
    "        else:\n",
    "            relevant_data = self.data\n",
    "\n",
    "        text_parts = []\n",
    "        for key, value in relevant_data.items():\n",
    "            if pd.notna(value) and value != '':\n",
    "                key_formatted = key.replace('_', ' ').title()\n",
    "                text_parts.append(f\"{key_formatted}: {value}\")\n",
    "\n",
    "        return \". \".join(text_parts) + \".\"\n",
    "\n",
    "    def get_metadata(self) -> Dict[str, Any]:\n",
    "        return self.data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56899464",
   "metadata": {},
   "source": [
    "# VectorStore Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aef25294",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    def __init__(self, embedding_model: str = 'all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(embedding_model)\n",
    "        self.dimension = self.model.get_sentence_embedding_dimension()\n",
    "        self.index = faiss.IndexFlatIP(self.dimension)\n",
    "        self.records: List[GenericRecord] = []\n",
    "        self.texts: List[str] = []\n",
    "        self.metadata: List[Dict] = []\n",
    "        self.text_columns: Optional[List[str]] = None\n",
    "\n",
    "    def add_records(self, records: List[GenericRecord], text_columns: Optional[List[str]] = None):\n",
    "        self.text_columns = text_columns\n",
    "        texts = [record.to_text(text_columns) for record in records]\n",
    "        embeddings = self.model.encode(texts, normalize_embeddings=True, show_progress_bar=False)\n",
    "\n",
    "        self.index.add(embeddings.astype('float32'))\n",
    "        self.records.extend(records)\n",
    "        self.texts.extend(texts)\n",
    "        self.metadata.extend([record.get_metadata() for record in records])\n",
    "\n",
    "    def search(self, query: str, k: int = 5, filters: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:\n",
    "        if len(self.records) == 0:\n",
    "            return []\n",
    "\n",
    "        query_embedding = self.model.encode([query], normalize_embeddings=True)\n",
    "        search_k = min(k * 5, len(self.records))\n",
    "        distances, indices = self.index.search(query_embedding.astype('float32'), search_k)\n",
    "\n",
    "        results = []\n",
    "        for dist, idx in zip(distances[0], indices[0]):\n",
    "            if idx < len(self.records):\n",
    "                record = self.records[idx]\n",
    "                metadata = self.metadata[idx]\n",
    "\n",
    "                if filters and not self._matches_filters(metadata, filters):\n",
    "                    continue\n",
    "\n",
    "                results.append({\n",
    "                    'primary_key': record.primary_key,\n",
    "                    'similarity_score': float(dist),\n",
    "                    'record': record.data\n",
    "                })\n",
    "\n",
    "                if len(results) >= k:\n",
    "                    break\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _matches_filters(self, metadata: Dict, filters: Dict) -> bool:\n",
    "        for key, value in filters.items():\n",
    "            if key not in metadata:\n",
    "                continue\n",
    "\n",
    "            meta_value = metadata[key]\n",
    "\n",
    "            if isinstance(value, list):\n",
    "                if meta_value not in value:\n",
    "                    return False\n",
    "\n",
    "            elif isinstance(value, tuple) and len(value) == 2:\n",
    "                operator, threshold = value\n",
    "                try:\n",
    "                    meta_value = float(meta_value)\n",
    "                    threshold = float(threshold)\n",
    "                except (ValueError, TypeError):\n",
    "                    return False\n",
    "\n",
    "                if operator == '>' and not meta_value > threshold:\n",
    "                    return False\n",
    "                elif operator == '>=' and not meta_value >= threshold:\n",
    "                    return False\n",
    "                elif operator == '<' and not meta_value < threshold:\n",
    "                    return False\n",
    "                elif operator == '<=' and not meta_value <= threshold:\n",
    "                    return False\n",
    "                elif operator == '==' and not meta_value == threshold:\n",
    "                    return False\n",
    "                elif operator == '!=' and not meta_value != threshold:\n",
    "                    return False\n",
    "            else:\n",
    "                if str(metadata[key]).lower() != str(value).lower():\n",
    "                    return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def save(self, path: str):\n",
    "        save_path = Path(path)\n",
    "        save_path.mkdir(parents=True, exist_ok=True)\n",
    "        faiss.write_index(self.index, str(save_path / \"index.faiss\"))\n",
    "        with open(save_path / \"data.pkl\", 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'records': self.records,\n",
    "                'texts': self.texts,\n",
    "                'metadata': self.metadata,\n",
    "                'text_columns': self.text_columns\n",
    "            }, f)\n",
    "\n",
    "    def load(self, path: str):\n",
    "        load_path = Path(path)\n",
    "        self.index = faiss.read_index(str(load_path / \"index.faiss\"))\n",
    "        with open(load_path / \"data.pkl\", 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            self.records = data['records']\n",
    "            self.texts = data['texts']\n",
    "            self.metadata = data['metadata']\n",
    "            self.text_columns = data.get('text_columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc85791f",
   "metadata": {},
   "source": [
    "# UniversalDataIndexer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d24d04d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniversalDataIndexer:\n",
    "    def __init__(self, embedding_model: str = 'all-MiniLM-L6-v2'):\n",
    "        self.stores: Dict[str, VectorStore] = {}\n",
    "        self.embedding_model = embedding_model\n",
    "        self.csv_schemas: Dict[str, Dict] = {}\n",
    "\n",
    "    def index_csv(self,\n",
    "                  csv_path: str,\n",
    "                  store_name: str,\n",
    "                  primary_key_column: Optional[str] = None,\n",
    "                  text_columns: Optional[List[str]] = None,\n",
    "                  exclude_columns: Optional[List[str]] = None):\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        if primary_key_column is None:\n",
    "            primary_key_column = df.columns[0]\n",
    "\n",
    "        if exclude_columns:\n",
    "            df = df.drop(columns=exclude_columns, errors='ignore')\n",
    "\n",
    "        self.csv_schemas[store_name] = {\n",
    "            'columns': list(df.columns),\n",
    "            'primary_key': primary_key_column,\n",
    "            'text_columns': text_columns\n",
    "        }\n",
    "\n",
    "        records = []\n",
    "        for _, row in df.iterrows():\n",
    "            data = {}\n",
    "            for col in df.columns:\n",
    "                value = row[col]\n",
    "                data[col] = '' if pd.isna(value) else value\n",
    "\n",
    "            record = GenericRecord(data=data, primary_key=str(row[primary_key_column]))\n",
    "            records.append(record)\n",
    "\n",
    "        store = VectorStore(self.embedding_model)\n",
    "        store.add_records(records, text_columns)\n",
    "        self.stores[store_name] = store\n",
    "\n",
    "        print(f\"✓ Indexed {len(records)} records from '{csv_path}' into '{store_name}' store\")\n",
    "\n",
    "    def search(self, store_name: str, query: str, k: int = 5, filters: Optional[Dict] = None) -> List[Dict]:\n",
    "        if store_name not in self.stores:\n",
    "            raise ValueError(f\"Store '{store_name}' not found. Available: {list(self.stores.keys())}\")\n",
    "        return self.stores[store_name].search(query, k, filters)\n",
    "\n",
    "    def get_schema(self, store_name: str) -> Dict:\n",
    "        return self.csv_schemas.get(store_name, {})\n",
    "\n",
    "    def list_stores(self) -> List[str]:\n",
    "        return list(self.stores.keys())\n",
    "\n",
    "    def save_all(self, base_path: str = \"./vector_stores\"):\n",
    "        for store_name, store in self.stores.items():\n",
    "            store.save(f\"{base_path}/{store_name}\")\n",
    "\n",
    "        schema_path = Path(base_path) / \"schemas.json\"\n",
    "        schema_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(schema_path, 'w') as f:\n",
    "            json.dump(self.csv_schemas, f, indent=2)\n",
    "\n",
    "        print(f\"✓ Saved all stores to '{base_path}'\")\n",
    "\n",
    "    def load_all(self, base_path: str = \"./vector_stores\"):\n",
    "        base = Path(base_path)\n",
    "\n",
    "        schema_path = base / \"schemas.json\"\n",
    "        if schema_path.exists():\n",
    "            with open(schema_path, 'r') as f:\n",
    "                self.csv_schemas = json.load(f)\n",
    "\n",
    "        for store_dir in base.iterdir():\n",
    "            if store_dir.is_dir():\n",
    "                store_name = store_dir.name\n",
    "                store = VectorStore(self.embedding_model)\n",
    "                store.load(str(store_dir))\n",
    "                self.stores[store_name] = store\n",
    "\n",
    "        print(f\"✓ Loaded {len(self.stores)} stores from '{base_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003cd02b",
   "metadata": {},
   "source": [
    "# Using the Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2d79a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexer created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create the indexer\n",
    "indexer = UniversalDataIndexer()\n",
    "print(\"Indexer created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54579fd5",
   "metadata": {},
   "source": [
    "## Index all csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ceef33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.path.abspath(\"../data_directory/unclean_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3bb42a",
   "metadata": {},
   "source": [
    "1. Index Accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49cd6914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Indexed 85 records from '/Users/nissiotoo/Documents/Team2B/data_directory/unclean_data/accounts.csv' into 'accounts' store\n"
     ]
    }
   ],
   "source": [
    "indexer.index_csv(\n",
    "    csv_path=os.path.join(base_path, \"accounts.csv\"),\n",
    "    store_name=\"accounts\",\n",
    "    primary_key_column=\"account\",\n",
    "    text_columns=[\"sector\", \"subsidiary_of\", \"office_location\"]  # Columns for semantic search\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83fe959",
   "metadata": {},
   "source": [
    "2. Index Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3991f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Indexed 7 records from '/Users/nissiotoo/Documents/Team2B/data_directory/unclean_data/products.csv' into 'products' store\n"
     ]
    }
   ],
   "source": [
    "indexer.index_csv(\n",
    "    csv_path=os.path.join(base_path, \"products.csv\"),\n",
    "    store_name=\"products\",\n",
    "    primary_key_column=\"product\",\n",
    "    text_columns=[\"product\", \"series\"]  # Product name and series for search\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2541e7",
   "metadata": {},
   "source": [
    "3. Index Sales Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "183dbfc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Indexed 8800 records from '/Users/nissiotoo/Documents/Team2B/data_directory/unclean_data/sales_pipeline.csv' into 'pipeline' store\n"
     ]
    }
   ],
   "source": [
    "indexer.index_csv(\n",
    "    csv_path=os.path.join(base_path, \"sales_pipeline.csv\"),\n",
    "    store_name=\"pipeline\",\n",
    "    primary_key_column=\"opportunity_id\",\n",
    "    text_columns=[\"account\", \"product\", \"sales_agent\", \"deal_stage\"]  # Key fields for search\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0751b656",
   "metadata": {},
   "source": [
    "4. Index Sales Teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f011988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Indexed 35 records from '/Users/nissiotoo/Documents/Team2B/data_directory/unclean_data/sales_teams.csv' into 'teams' store\n"
     ]
    }
   ],
   "source": [
    "indexer.index_csv(\n",
    "    csv_path=os.path.join(base_path, \"sales_teams.csv\"),\n",
    "    store_name=\"teams\",\n",
    "    primary_key_column=\"sales_agent\",  # first column as key\n",
    "    text_columns=[\"sales_agent\", \"manager\", \"regional_office\"] # Key fields for search (all columns)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c92b37",
   "metadata": {},
   "source": [
    "Index Data Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97e73148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Indexed 21 records from '/Users/nissiotoo/Documents/Team2B/data_directory/unclean_data/data_dictionary.csv' into 'dictionary' store\n"
     ]
    }
   ],
   "source": [
    "indexer.index_csv(\n",
    "    csv_path=os.path.join(base_path, \"data_dictionary.csv\"),\n",
    "    store_name=\"dictionary\",\n",
    "    primary_key_column=\"Table\",  # first column as key\n",
    "    text_columns=[\"Table\", \"Field\", \"Description\"]  # Key fields for search (all columns)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ea08c9",
   "metadata": {},
   "source": [
    "## Verify the stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3eb70329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available stores: ['accounts', 'products', 'pipeline', 'teams', 'dictionary']\n",
      "\n",
      "ACCOUNTS:\n",
      "   Primary Key: account\n",
      "   Columns: ['account', 'sector', 'year_established', 'revenue', 'employees']...\n",
      "   Total Records: 85\n",
      "\n",
      "PRODUCTS:\n",
      "   Primary Key: product\n",
      "   Columns: ['product', 'series', 'sales_price']...\n",
      "   Total Records: 7\n",
      "\n",
      "PIPELINE:\n",
      "   Primary Key: opportunity_id\n",
      "   Columns: ['opportunity_id', 'sales_agent', 'product', 'account', 'deal_stage']...\n",
      "   Total Records: 8800\n",
      "\n",
      "TEAMS:\n",
      "   Primary Key: sales_agent\n",
      "   Columns: ['sales_agent', 'manager', 'regional_office']...\n",
      "   Total Records: 35\n",
      "\n",
      "DICTIONARY:\n",
      "   Primary Key: Table\n",
      "   Columns: ['Table', 'Field', 'Description']...\n",
      "   Total Records: 21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List all indexed stores\n",
    "stores = indexer.list_stores()\n",
    "print(f\"Available stores: {stores}\\n\")\n",
    "\n",
    "# Show schema for each store\n",
    "for store in stores:\n",
    "    schema = indexer.get_schema(store)\n",
    "    print(f\"{store.upper()}:\")\n",
    "    print(f\"   Primary Key: {schema['primary_key']}\")\n",
    "    print(f\"   Columns: {schema['columns'][:5]}...\")  # Show first 5 columns\n",
    "    print(f\"   Total Records: {len(indexer.stores[store].records)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba22fa02",
   "metadata": {},
   "source": [
    "# Search accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58c731c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEALTHCARE/TECH ACCOUNTS:\n",
      "\n",
      "1. Score: 0.510\n",
      "   Account: Zumgoity\n",
      "   Sector: medical\n",
      "   Revenue: 441.08\n",
      "   Employees: 1210\n",
      "\n",
      "2. Score: 0.510\n",
      "   Account: The New York Inquirer\n",
      "   Sector: medical\n",
      "   Revenue: 439.21\n",
      "   Employees: 792\n",
      "\n",
      "3. Score: 0.510\n",
      "   Account: Silis\n",
      "   Sector: medical\n",
      "   Revenue: 2818.38\n",
      "   Employees: 6290\n",
      "\n",
      "4. Score: 0.510\n",
      "   Account: Ron-tech\n",
      "   Sector: medical\n",
      "   Revenue: 3922.42\n",
      "   Employees: 6837\n",
      "\n",
      "5. Score: 0.510\n",
      "   Account: Lexiqvolax\n",
      "   Sector: medical\n",
      "   Revenue: 1618.89\n",
      "   Employees: 3889\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find accounts in specific sectors\n",
    "results = indexer.search(\n",
    "    store_name=\"accounts\",\n",
    "    query=\"healthcare technology companies\",\n",
    "    k=5\n",
    ")\n",
    "\n",
    "print(\"HEALTHCARE/TECH ACCOUNTS:\\n\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"{i}. Score: {result['similarity_score']:.3f}\")\n",
    "    print(f\"   Account: {result['record'].get('account', 'N/A')}\")\n",
    "    print(f\"   Sector: {result['record'].get('sector', 'N/A')}\")\n",
    "    print(f\"   Revenue: {result['record'].get('revenue', 'N/A')}\")\n",
    "    print(f\"   Employees: {result['record'].get('employees', 'N/A')}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fa8c81",
   "metadata": {},
   "source": [
    "# Search sales pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34f962f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIGH-VALUE DEALS:\n",
      "\n",
      "1. Score: 0.365\n",
      "   Account: Doncon\n",
      "   Product: MG Special\n",
      "   Close Value: 58.0\n",
      "   Stage: Won\n",
      "\n",
      "2. Score: 0.350\n",
      "   Account: Stanredtax\n",
      "   Product: MG Special\n",
      "   Close Value: 60.0\n",
      "   Stage: Won\n",
      "\n",
      "3. Score: 0.349\n",
      "   Account: Donquadtech\n",
      "   Product: MG Special\n",
      "   Close Value: 59.0\n",
      "   Stage: Won\n",
      "\n",
      "4. Score: 0.348\n",
      "   Account: Treequote\n",
      "   Product: MG Special\n",
      "   Close Value: 60.0\n",
      "   Stage: Won\n",
      "\n",
      "5. Score: 0.348\n",
      "   Account: Doncon\n",
      "   Product: GTX Basic\n",
      "   Close Value: 529.0\n",
      "   Stage: Won\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find high-value deals\n",
    "results = indexer.search(\n",
    "    store_name=\"pipeline\",\n",
    "    query=\"low value closed won deals\",\n",
    "    k=5\n",
    ")\n",
    "\n",
    "print(\"HIGH-VALUE DEALS:\\n\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"{i}. Score: {result['similarity_score']:.3f}\")\n",
    "    print(f\"   Account: {result['record'].get('account', 'N/A')}\")\n",
    "    print(f\"   Product: {result['record'].get('product', 'N/A')}\")\n",
    "    print(f\"   Close Value: {result['record'].get('close_value', 'N/A')}\")\n",
    "    print(f\"   Stage: {result['record'].get('deal_stage', 'N/A')}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaafa0c",
   "metadata": {},
   "source": [
    "# Search with filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "321726ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LARGE ENTERPRISE ACCOUNTS (Revenue > $1,000):\n",
      "\n",
      "1. Cheers\n",
      "   Sector: entertainment\n",
      "   Revenue: $4,269.9\n",
      "\n",
      "2. Zotware\n",
      "   Sector: software\n",
      "   Revenue: $4,478.47\n",
      "\n",
      "3. Kan-code\n",
      "   Sector: software\n",
      "   Revenue: $11,698.03\n",
      "\n",
      "4. Dontechi\n",
      "   Sector: software\n",
      "   Revenue: $4,618.0\n",
      "\n",
      "5. Scotfind\n",
      "   Sector: software\n",
      "   Revenue: $6,354.87\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find accounts with revenue > $1,000\n",
    "results = indexer.search(\n",
    "    store_name=\"accounts\",\n",
    "    query=\"large enterprise accounts\",\n",
    "    k=5,\n",
    "    filters={\"revenue\": ('>', 1000)}  # Filter for revenue > $1,000\n",
    ")\n",
    "\n",
    "print(\"LARGE ENTERPRISE ACCOUNTS (Revenue > $1,000):\\n\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"{i}. {result['record'].get('account', 'N/A')}\")\n",
    "    print(f\"   Sector: {result['record'].get('sector', 'N/A')}\")\n",
    "    print(f\"   Revenue: ${result['record'].get('revenue', 'N/A'):,}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a929fed3",
   "metadata": {},
   "source": [
    "# Search sales teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0dfd488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEST COAST SALES TEAM:\n",
      "\n",
      "1. Agent: Carol Thompson\n",
      "   Manager: Celia Rouche\n",
      "   Office: West\n",
      "\n",
      "2. Agent: Elizabeth Anderson\n",
      "   Manager: Cara Losch\n",
      "   Office: East\n",
      "\n",
      "3. Agent: Markita Hansen\n",
      "   Manager: Celia Rouche\n",
      "   Office: West\n",
      "\n",
      "4. Agent: Wilburn Farren\n",
      "   Manager: Cara Losch\n",
      "   Office: East\n",
      "\n",
      "5. Agent: Vicki Laflamme\n",
      "   Manager: Celia Rouche\n",
      "   Office: West\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find sales agents by region\n",
    "results = indexer.search(\n",
    "    store_name=\"teams\",\n",
    "    query=\"west coast sales representatives\",\n",
    "    k=5\n",
    ")\n",
    "\n",
    "print(\"WEST COAST SALES TEAM:\\n\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"{i}. Agent: {result['record'].get('sales_agent', 'N/A')}\")\n",
    "    print(f\"   Manager: {result['record'].get('manager', 'N/A')}\")\n",
    "    print(f\"   Office: {result['record'].get('regional_office', 'N/A')}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c23dc0",
   "metadata": {},
   "source": [
    "# Cross-Reference Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7cc37420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEALS FOR: sonron\n",
      "\n",
      "1. Product: MG Advanced\n",
      "   Close Value: 3250.0\n",
      "   Stage: Won\n",
      "\n",
      "2. Product: MG Advanced\n",
      "   Close Value: 3527.0\n",
      "   Stage: Won\n",
      "\n",
      "3. Product: MG Advanced\n",
      "   Close Value: 3689.0\n",
      "   Stage: Won\n",
      "\n",
      "4. Product: MG Advanced\n",
      "   Close Value: 3484.0\n",
      "   Stage: Won\n",
      "\n",
      "5. Product: GTX Plus Pro\n",
      "   Close Value: 5816.0\n",
      "   Stage: Won\n",
      "\n",
      "6. Product: GTX Plus Pro\n",
      "   Close Value: 4631.0\n",
      "   Stage: Won\n",
      "\n",
      "7. Product: GTXPro\n",
      "   Close Value: 4936.0\n",
      "   Stage: Won\n",
      "\n",
      "8. Product: GTXPro\n",
      "   Close Value: 4683.0\n",
      "   Stage: Won\n",
      "\n",
      "9. Product: MG Special\n",
      "   Close Value: 0.0\n",
      "   Stage: Lost\n",
      "\n",
      "10. Product: GTX Basic\n",
      "   Close Value: 432.0\n",
      "   Stage: Won\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find which products a specific account bought\n",
    "account_name = \"sonron\"  # Search any account name from accounts.csv\n",
    "\n",
    "# First find the account\n",
    "account_results = indexer.search(\n",
    "    store_name=\"accounts\",\n",
    "    query=account_name,\n",
    "    k=1\n",
    ")\n",
    "\n",
    "if account_results:\n",
    "    print(f\"DEALS FOR: {account_name}\\n\")\n",
    "    \n",
    "    # Then find deals for that account\n",
    "    pipeline_results = indexer.search(\n",
    "        store_name=\"pipeline\",\n",
    "        query=account_name,\n",
    "        k=10\n",
    "    )\n",
    "    \n",
    "    for i, result in enumerate(pipeline_results, 1):\n",
    "        print(f\"{i}. Product: {result['record'].get('product', 'N/A')}\")\n",
    "        print(f\"   Close Value: {result['record'].get('close_value', 'N/A')}\")\n",
    "        print(f\"   Stage: {result['record'].get('deal_stage', 'N/A')}\\n\")\n",
    "else:\n",
    "    print(f\"Account '{account_name}' not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec0349e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
