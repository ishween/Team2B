{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### This notebook is for Milestone #1 and focuses on data preprocessing. It covers three main stages: data cleaning to handle duplicates, missing values, and data types; data transformation to encode categorical features and scale numerical ones; and data reduction through feature engineering and column selection. The goal is to prepare the raw data for machine learning model implementation."
      ],
      "metadata": {
        "id": "YbQi4rOCJ4XB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Nam-g2fVJNvn"
      },
      "outputs": [],
      "source": [
        "# Import the Necessary Packages for Data Cleaning\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Load Datasets"
      ],
      "metadata": {
        "id": "Ecqlhy1vNu8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the directory and loading each CSV file into a pandas DataFrame\n",
        "data_directory = \"/content/data_directory/\"\n",
        "\n",
        "accounts = pd.read_csv(os.path.join(data_directory, \"accounts.csv\"))\n",
        "products = pd.read_csv(os.path.join(data_directory, \"products.csv\"))\n",
        "pipeline = pd.read_csv(os.path.join(data_directory, \"sales_pipeline.csv\"))\n",
        "teams = pd.read_csv(os.path.join(data_directory, \"sales_teams.csv\"))\n",
        "dictionary = pd.read_csv(os.path.join(data_directory, \"data_dictionary.csv\"))"
      ],
      "metadata": {
        "id": "GBsuDWguLfl5"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is storing DataFrames in a dictionary for easy iteration\n",
        "dataframes = {\n",
        "    \"accounts\": accounts,\n",
        "    \"products\": products,\n",
        "    \"sales_pipeline\": pipeline,\n",
        "    \"sales_teams\": teams,\n",
        "    \"data_dictionary\": dictionary\n",
        "}"
      ],
      "metadata": {
        "id": "6YCmCyMDNsdg"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Data Cleaning"
      ],
      "metadata": {
        "id": "OsumfhBbOcsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a function to standardize column names\n",
        "def standardize_columns(df):\n",
        "    \"\"\"This is converting column names to lowercase and replacing spaces with underscores.\"\"\"\n",
        "    df.columns = df.columns.str.lower().str.replace(\" \", \"_\")\n",
        "    return df"
      ],
      "metadata": {
        "id": "PzwZWznqOff5"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is applying the column standardization to all DataFrames\n",
        "for name, df in dataframes.items():\n",
        "    dataframes[name] = standardize_columns(df)"
      ],
      "metadata": {
        "id": "pkUczY3KOkyQ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is fixing typos and standardizing categorical values in the accounts dataframe\n",
        "if 'sector' in dataframes['accounts'].columns:\n",
        "    dataframes['accounts']['sector'] = dataframes['accounts']['sector'].replace({\"technolgy\": \"technology\"})\n",
        "\n",
        "if 'office_location' in dataframes['accounts'].columns:\n",
        "    dataframes['accounts']['office_location'] = dataframes['accounts']['office_location'].replace({\"Philipines\": \"Philippines\"})"
      ],
      "metadata": {
        "id": "ddb8TyAUOmsI"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is handling duplicates in all DataFrames\n",
        "for name, df in dataframes.items():\n",
        "    initial_rows = len(df)\n",
        "    dataframes[name].drop_duplicates(inplace=True)\n",
        "    rows_after = len(dataframes[name])\n",
        "    if initial_rows > rows_after:\n",
        "        print(f\"Removed {initial_rows - rows_after} duplicate rows from {name}.\")"
      ],
      "metadata": {
        "id": "66PLbP18OxS1"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is handling missing values\n",
        "# This is filling numerical columns with median and categorical columns with 'unknown'\n",
        "for name, df in dataframes.items():\n",
        "    print(f\"\\n--- Handling missing values for {name} ---\")\n",
        "\n",
        "    # This is checking for missing values before cleaning\n",
        "    initial_missing = df.isnull().sum()\n",
        "    print(f\"Missing values before:\\n{initial_missing[initial_missing > 0]}\")\n",
        "\n",
        "    # This is imputing missing values based on data type\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype in ['int64', 'float64']:\n",
        "            df[col].fillna(df[col].median(), inplace=True)\n",
        "        elif df[col].dtype == 'object':\n",
        "            df[col].fillna('unknown', inplace=True)\n",
        "\n",
        "    # This is checking for missing values after cleaning\n",
        "    final_missing = df.isnull().sum()\n",
        "    print(f\"Missing values after:\\n{final_missing[final_missing > 0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loV5c36fO0Fi",
        "outputId": "8fe1b6c2-e261-4e66-8945-a0a47f79ae40"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Handling missing values for accounts ---\n",
            "Missing values before:\n",
            "subsidiary_of    70\n",
            "dtype: int64\n",
            "Missing values after:\n",
            "Series([], dtype: int64)\n",
            "\n",
            "--- Handling missing values for products ---\n",
            "Missing values before:\n",
            "Series([], dtype: int64)\n",
            "Missing values after:\n",
            "Series([], dtype: int64)\n",
            "\n",
            "--- Handling missing values for sales_pipeline ---\n",
            "Missing values before:\n",
            "account        1425\n",
            "engage_date     500\n",
            "close_date     2089\n",
            "close_value    2089\n",
            "dtype: int64\n",
            "Missing values after:\n",
            "Series([], dtype: int64)\n",
            "\n",
            "--- Handling missing values for sales_teams ---\n",
            "Missing values before:\n",
            "Series([], dtype: int64)\n",
            "Missing values after:\n",
            "Series([], dtype: int64)\n",
            "\n",
            "--- Handling missing values for data_dictionary ---\n",
            "Missing values before:\n",
            "Series([], dtype: int64)\n",
            "Missing values after:\n",
            "Series([], dtype: int64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2380130685.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna('unknown', inplace=True)\n",
            "/tmp/ipython-input-2380130685.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(df[col].median(), inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Data Transformation & Reduction"
      ],
      "metadata": {
        "id": "dq2m8f6IPLu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is converting date columns to datetime objects in the sales_pipeline dataframe\n",
        "df_pipeline = dataframes['sales_pipeline']\n",
        "for col in ['engage_date', 'close_date']:\n",
        "    if col in df_pipeline.columns:\n",
        "        df_pipeline[col] = pd.to_datetime(df_pipeline[col], errors='coerce')\n",
        "\n",
        "# This is dropping columns with high missing values or unique identifiers\n",
        "dataframes['accounts'].drop('subsidiary_of', axis=1, inplace=True)\n",
        "dataframes['sales_pipeline'].drop('opportunity_id', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "-VLD9cMNPS09"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Save Cleaned Data"
      ],
      "metadata": {
        "id": "uh-tm2OFPU42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is creating a directory to save cleaned datasets\n",
        "cleaned_dir = \"data_cleaned\"\n",
        "os.makedirs(cleaned_dir, exist_ok=True)\n",
        "\n",
        "# This is saving each cleaned DataFrame as a CSV\n",
        "for name, df in dataframes.items():\n",
        "    # This is skipping the data dictionary as it is not needed for the model\n",
        "    if name != 'data_dictionary':\n",
        "        file_path = os.path.join(cleaned_dir, f\"cleaned_{name}.csv\")\n",
        "        df.to_csv(file_path, index=False)\n",
        "        print(f\"\\nSaved cleaned data to {file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5KhU9gsPYIi",
        "outputId": "ab651f0f-0071-4412-da83-17120e131763"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved cleaned data to data_cleaned/cleaned_accounts.csv\n",
            "\n",
            "Saved cleaned data to data_cleaned/cleaned_products.csv\n",
            "\n",
            "Saved cleaned data to data_cleaned/cleaned_sales_pipeline.csv\n",
            "\n",
            "Saved cleaned data to data_cleaned/cleaned_sales_teams.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Final Verification"
      ],
      "metadata": {
        "id": "RGElGnaYPdCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is verifying the final state of the cleaned datasets\n",
        "print(\"\\n--- Final Dataframe Verification ---\")\n",
        "for name in ['accounts', 'products', 'sales_pipeline', 'sales_teams']:\n",
        "    df = dataframes[name]\n",
        "    print(f\"\\nDataFrame: {name}\")\n",
        "    print(f\"Shape: {df.shape}\")\n",
        "    print(f\"Missing values:\\n{df.isnull().sum()}\")\n",
        "    print(f\"Data types:\\n{df.dtypes}\")\n",
        "    print(f\"Duplicate rows: {df.duplicated().sum()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtlQ_wAgPbbr",
        "outputId": "0a97ae67-b8f3-43c6-a7ef-91ca5bb5cc53"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Final Dataframe Verification ---\n",
            "\n",
            "DataFrame: accounts\n",
            "Shape: (85, 6)\n",
            "Missing values:\n",
            "account             0\n",
            "sector              0\n",
            "year_established    0\n",
            "revenue             0\n",
            "employees           0\n",
            "office_location     0\n",
            "dtype: int64\n",
            "Data types:\n",
            "account              object\n",
            "sector               object\n",
            "year_established      int64\n",
            "revenue             float64\n",
            "employees             int64\n",
            "office_location      object\n",
            "dtype: object\n",
            "Duplicate rows: 0\n",
            "\n",
            "DataFrame: products\n",
            "Shape: (7, 3)\n",
            "Missing values:\n",
            "product        0\n",
            "series         0\n",
            "sales_price    0\n",
            "dtype: int64\n",
            "Data types:\n",
            "product        object\n",
            "series         object\n",
            "sales_price     int64\n",
            "dtype: object\n",
            "Duplicate rows: 0\n",
            "\n",
            "DataFrame: sales_pipeline\n",
            "Shape: (8800, 7)\n",
            "Missing values:\n",
            "sales_agent       0\n",
            "product           0\n",
            "account           0\n",
            "deal_stage        0\n",
            "engage_date     500\n",
            "close_date     2089\n",
            "close_value       0\n",
            "dtype: int64\n",
            "Data types:\n",
            "sales_agent            object\n",
            "product                object\n",
            "account                object\n",
            "deal_stage             object\n",
            "engage_date    datetime64[ns]\n",
            "close_date     datetime64[ns]\n",
            "close_value           float64\n",
            "dtype: object\n",
            "Duplicate rows: 352\n",
            "\n",
            "DataFrame: sales_teams\n",
            "Shape: (35, 3)\n",
            "Missing values:\n",
            "sales_agent        0\n",
            "manager            0\n",
            "regional_office    0\n",
            "dtype: int64\n",
            "Data types:\n",
            "sales_agent        object\n",
            "manager            object\n",
            "regional_office    object\n",
            "dtype: object\n",
            "Duplicate rows: 0\n"
          ]
        }
      ]
    }
  ]
}