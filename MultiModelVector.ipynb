{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-08T01:45:27.178040Z",
     "start_time": "2025-11-08T01:45:19.736218Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ],
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\nFailed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\machineLearningEnv\\Lib\\site-packages\\transformers\\activations_tf.py:22\u001B[39m\n\u001B[32m     21\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m22\u001B[39m     \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtf_keras\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mkeras\u001B[39;00m\n\u001B[32m     23\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mModuleNotFoundError\u001B[39;00m, \u001B[38;5;167;01mImportError\u001B[39;00m):\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'tf_keras'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\machineLearningEnv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1603\u001B[39m, in \u001B[36m_LazyModule._get_module\u001B[39m\u001B[34m(self, module_name)\u001B[39m\n\u001B[32m   1602\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1603\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m importlib.import_module(\u001B[33m\"\u001B[39m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m + module_name, \u001B[38;5;28mself\u001B[39m.\u001B[34m__name__\u001B[39m)\n\u001B[32m   1604\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\machineLearningEnv\\Lib\\importlib\\__init__.py:126\u001B[39m, in \u001B[36mimport_module\u001B[39m\u001B[34m(name, package)\u001B[39m\n\u001B[32m    125\u001B[39m         level += \u001B[32m1\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m126\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m _bootstrap._gcd_import(name[level:], package, level)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:1204\u001B[39m, in \u001B[36m_gcd_import\u001B[39m\u001B[34m(name, package, level)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:1176\u001B[39m, in \u001B[36m_find_and_load\u001B[39m\u001B[34m(name, import_)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:1147\u001B[39m, in \u001B[36m_find_and_load_unlocked\u001B[39m\u001B[34m(name, import_)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:690\u001B[39m, in \u001B[36m_load_unlocked\u001B[39m\u001B[34m(spec)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap_external>:940\u001B[39m, in \u001B[36mexec_module\u001B[39m\u001B[34m(self, module)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:241\u001B[39m, in \u001B[36m_call_with_frames_removed\u001B[39m\u001B[34m(f, *args, **kwds)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\machineLearningEnv\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:38\u001B[39m\n\u001B[32m     37\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m DataCollatorWithPadding, DefaultDataCollator\n\u001B[32m---> \u001B[39m\u001B[32m38\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mactivations_tf\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m get_tf_activation\n\u001B[32m     39\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mconfiguration_utils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m PretrainedConfig\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\machineLearningEnv\\Lib\\site-packages\\transformers\\activations_tf.py:27\u001B[39m\n\u001B[32m     26\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m parse(keras.__version__).major > \u001B[32m2\u001B[39m:\n\u001B[32m---> \u001B[39m\u001B[32m27\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m     28\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mYour currently installed version of Keras is Keras 3, but this is not yet supported in \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     29\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mTransformers. Please install the backwards-compatible tf-keras package with \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     30\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33m`pip install tf-keras`.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     31\u001B[39m         )\n\u001B[32m     34\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_gelu\u001B[39m(x):\n",
      "\u001B[31mValueError\u001B[39m: Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\machineLearningEnv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1603\u001B[39m, in \u001B[36m_LazyModule._get_module\u001B[39m\u001B[34m(self, module_name)\u001B[39m\n\u001B[32m   1602\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1603\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m importlib.import_module(\u001B[33m\"\u001B[39m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m + module_name, \u001B[38;5;28mself\u001B[39m.\u001B[34m__name__\u001B[39m)\n\u001B[32m   1604\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\machineLearningEnv\\Lib\\importlib\\__init__.py:126\u001B[39m, in \u001B[36mimport_module\u001B[39m\u001B[34m(name, package)\u001B[39m\n\u001B[32m    125\u001B[39m         level += \u001B[32m1\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m126\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m _bootstrap._gcd_import(name[level:], package, level)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:1204\u001B[39m, in \u001B[36m_gcd_import\u001B[39m\u001B[34m(name, package, level)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:1176\u001B[39m, in \u001B[36m_find_and_load\u001B[39m\u001B[34m(name, import_)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:1147\u001B[39m, in \u001B[36m_find_and_load_unlocked\u001B[39m\u001B[34m(name, import_)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:690\u001B[39m, in \u001B[36m_load_unlocked\u001B[39m\u001B[34m(spec)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap_external>:940\u001B[39m, in \u001B[36mexec_module\u001B[39m\u001B[34m(self, module)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:241\u001B[39m, in \u001B[36m_call_with_frames_removed\u001B[39m\u001B[34m(f, *args, **kwds)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\machineLearningEnv\\Lib\\site-packages\\transformers\\integrations\\integration_utils.py:36\u001B[39m\n\u001B[32m     34\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpackaging\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mversion\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m36\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m PreTrainedModel, TFPreTrainedModel\n\u001B[32m     37\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m __version__ \u001B[38;5;28;01mas\u001B[39;00m version\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:1229\u001B[39m, in \u001B[36m_handle_fromlist\u001B[39m\u001B[34m(module, fromlist, import_, recursive)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\machineLearningEnv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1593\u001B[39m, in \u001B[36m_LazyModule.__getattr__\u001B[39m\u001B[34m(self, name)\u001B[39m\n\u001B[32m   1592\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._class_to_module.keys():\n\u001B[32m-> \u001B[39m\u001B[32m1593\u001B[39m     module = \u001B[38;5;28mself\u001B[39m._get_module(\u001B[38;5;28mself\u001B[39m._class_to_module[name])\n\u001B[32m   1594\u001B[39m     value = \u001B[38;5;28mgetattr\u001B[39m(module, name)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\machineLearningEnv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1605\u001B[39m, in \u001B[36m_LazyModule._get_module\u001B[39m\u001B[34m(self, module_name)\u001B[39m\n\u001B[32m   1604\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m-> \u001B[39m\u001B[32m1605\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m   1606\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mFailed to import \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodule_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m because of the following error (look up to see its\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1607\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m traceback):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m   1608\u001B[39m     ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01me\u001B[39;00m\n",
      "\u001B[31mRuntimeError\u001B[39m: Failed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 6\u001B[39m\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mjson\u001B[39;00m\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mdataclasses\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m dataclass\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msentence_transformers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m SentenceTransformer\n\u001B[32m      7\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mfaiss\u001B[39;00m\n\u001B[32m      8\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpathlib\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Path\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\machineLearningEnv\\Lib\\site-packages\\sentence_transformers\\__init__.py:14\u001B[39m\n\u001B[32m      7\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mos\u001B[39;00m\n\u001B[32m      9\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msentence_transformers\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mbackend\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m     10\u001B[39m     export_dynamic_quantized_onnx_model,\n\u001B[32m     11\u001B[39m     export_optimized_onnx_model,\n\u001B[32m     12\u001B[39m     export_static_quantized_openvino_model,\n\u001B[32m     13\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m14\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msentence_transformers\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcross_encoder\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m     15\u001B[39m     CrossEncoder,\n\u001B[32m     16\u001B[39m     CrossEncoderModelCardData,\n\u001B[32m     17\u001B[39m     CrossEncoderTrainer,\n\u001B[32m     18\u001B[39m     CrossEncoderTrainingArguments,\n\u001B[32m     19\u001B[39m )\n\u001B[32m     20\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msentence_transformers\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdatasets\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ParallelSentencesDataset, SentencesDataset\n\u001B[32m     21\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msentence_transformers\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mLoggingHandler\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m LoggingHandler\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\machineLearningEnv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\__init__.py:3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m__future__\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m annotations\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mCrossEncoder\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m CrossEncoder\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mmodel_card\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m CrossEncoderModelCardData\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mtrainer\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m CrossEncoderTrainer\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\machineLearningEnv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:31\u001B[39m\n\u001B[32m     28\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtyping_extensions\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m deprecated\n\u001B[32m     30\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msentence_transformers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m __version__\n\u001B[32m---> \u001B[39m\u001B[32m31\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msentence_transformers\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcross_encoder\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mfit_mixin\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m FitMixin\n\u001B[32m     32\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msentence_transformers\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcross_encoder\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mmodel_card\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m CrossEncoderModelCardData, generate_model_card\n\u001B[32m     33\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msentence_transformers\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcross_encoder\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutil\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m     34\u001B[39m     cross_encoder_init_args_decorator,\n\u001B[32m     35\u001B[39m     cross_encoder_predict_rank_args_decorator,\n\u001B[32m     36\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\machineLearningEnv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\fit_mixin.py:20\u001B[39m\n\u001B[32m     18\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msentence_transformers\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcross_encoder\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mtraining_args\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m CrossEncoderTrainingArguments\n\u001B[32m     19\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msentence_transformers\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdatasets\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mNoDuplicatesDataLoader\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m NoDuplicatesDataLoader\n\u001B[32m---> \u001B[39m\u001B[32m20\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msentence_transformers\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdatasets\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mSentenceLabelDataset\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m SentenceLabelDataset\n\u001B[32m     21\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msentence_transformers\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mevaluation\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mSentenceEvaluator\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m SentenceEvaluator\n\u001B[32m     22\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msentence_transformers\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mreaders\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m InputExample\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\machineLearningEnv\\Lib\\site-packages\\sentence_transformers\\datasets\\__init__.py:13\u001B[39m\n\u001B[32m     11\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mDenoisingAutoEncoderDataset\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m DenoisingAutoEncoderDataset\n\u001B[32m     12\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mNoDuplicatesDataLoader\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m NoDuplicatesDataLoader\n\u001B[32m---> \u001B[39m\u001B[32m13\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mParallelSentencesDataset\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ParallelSentencesDataset\n\u001B[32m     14\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mSentenceLabelDataset\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m SentenceLabelDataset\n\u001B[32m     15\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mSentencesDataset\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m SentencesDataset\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\machineLearningEnv\\Lib\\site-packages\\sentence_transformers\\datasets\\ParallelSentencesDataset.py:19\u001B[39m\n\u001B[32m     15\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mrandom\u001B[39;00m\n\u001B[32m     17\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdata\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Dataset\n\u001B[32m---> \u001B[39m\u001B[32m19\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msentence_transformers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m SentenceTransformer\n\u001B[32m     20\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msentence_transformers\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mreaders\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m InputExample\n\u001B[32m     22\u001B[39m logger = logging.getLogger(\u001B[34m__name__\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\machineLearningEnv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:34\u001B[39m\n\u001B[32m     31\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtransformers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m is_torch_npu_available\n\u001B[32m     32\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtransformers\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdynamic_module_utils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m get_class_from_dynamic_module, get_relative_import_files\n\u001B[32m---> \u001B[39m\u001B[32m34\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msentence_transformers\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mmodel_card\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m SentenceTransformerModelCardData, generate_model_card\n\u001B[32m     35\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msentence_transformers\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01msimilarity_functions\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m SimilarityFunction\n\u001B[32m     37\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m __MODEL_HUB_ORGANIZATION__, __version__\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\machineLearningEnv\\Lib\\site-packages\\sentence_transformers\\model_card.py:25\u001B[39m\n\u001B[32m     23\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtqdm\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mautonotebook\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m tqdm\n\u001B[32m     24\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtransformers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m TrainerCallback\n\u001B[32m---> \u001B[39m\u001B[32m25\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtransformers\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mintegrations\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m CodeCarbonCallback\n\u001B[32m     26\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtransformers\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mmodelcard\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m make_markdown_table\n\u001B[32m     27\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtransformers\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mtrainer_callback\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m TrainerControl, TrainerState\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:1229\u001B[39m, in \u001B[36m_handle_fromlist\u001B[39m\u001B[34m(module, fromlist, import_, recursive)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\machineLearningEnv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1593\u001B[39m, in \u001B[36m_LazyModule.__getattr__\u001B[39m\u001B[34m(self, name)\u001B[39m\n\u001B[32m   1591\u001B[39m     value = \u001B[38;5;28mself\u001B[39m._get_module(name)\n\u001B[32m   1592\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._class_to_module.keys():\n\u001B[32m-> \u001B[39m\u001B[32m1593\u001B[39m     module = \u001B[38;5;28mself\u001B[39m._get_module(\u001B[38;5;28mself\u001B[39m._class_to_module[name])\n\u001B[32m   1594\u001B[39m     value = \u001B[38;5;28mgetattr\u001B[39m(module, name)\n\u001B[32m   1595\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\miniconda3\\envs\\machineLearningEnv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1605\u001B[39m, in \u001B[36m_LazyModule._get_module\u001B[39m\u001B[34m(self, module_name)\u001B[39m\n\u001B[32m   1603\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m importlib.import_module(\u001B[33m\"\u001B[39m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m + module_name, \u001B[38;5;28mself\u001B[39m.\u001B[34m__name__\u001B[39m)\n\u001B[32m   1604\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m-> \u001B[39m\u001B[32m1605\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m   1606\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mFailed to import \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodule_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m because of the following error (look up to see its\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1607\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m traceback):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m   1608\u001B[39m     ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01me\u001B[39;00m\n",
      "\u001B[31mRuntimeError\u001B[39m: Failed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\nFailed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`."
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class GenericRecord:\n",
    "    data: Dict[str, Any]\n",
    "    primary_key: str\n",
    "\n",
    "    def to_text(self, text_columns: Optional[List[str]] = None) -> str:\n",
    "        if text_columns:\n",
    "            relevant_data = {k: v for k, v in self.data.items() if k in text_columns}\n",
    "        else:\n",
    "            relevant_data = self.data\n",
    "\n",
    "        text_parts = []\n",
    "        for key, value in relevant_data.items():\n",
    "            if pd.notna(value) and value != '':\n",
    "                key_formatted = key.replace('_', ' ').title()\n",
    "                text_parts.append(f\"{key_formatted}: {value}\")\n",
    "\n",
    "        return \". \".join(text_parts) + \".\"\n",
    "\n",
    "    def get_metadata(self) -> Dict[str, Any]:\n",
    "        return self.data.copy()\n",
    "\n"
   ],
   "id": "4dcd3205487dc45e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class VectorStore:\n",
    "\n",
    "    def __init__(self, embedding_model: str = 'all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(embedding_model)\n",
    "        self.dimension = self.model.get_sentence_embedding_dimension()\n",
    "        self.index = faiss.IndexFlatIP(self.dimension)\n",
    "        self.records: List[GenericRecord] = []\n",
    "        self.texts: List[str] = []\n",
    "        self.metadata: List[Dict] = []\n",
    "        self.text_columns: Optional[List[str]] = None\n",
    "\n",
    "    def add_records(self, records: List[GenericRecord], text_columns: Optional[List[str]] = None):\n",
    "        self.text_columns = text_columns\n",
    "        texts = [record.to_text(text_columns) for record in records]\n",
    "        embeddings = self.model.encode(texts, normalize_embeddings=True, show_progress_bar=False)\n",
    "\n",
    "        self.index.add(embeddings.astype('float32'))\n",
    "        self.records.extend(records)\n",
    "        self.texts.extend(texts)\n",
    "        self.metadata.extend([record.get_metadata() for record in records])\n",
    "\n",
    "    def search(self, query: str, k: int = 5, filters: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:\n",
    "        if len(self.records) == 0:\n",
    "            return []\n",
    "\n",
    "        query_embedding = self.model.encode([query], normalize_embeddings=True)\n",
    "        search_k = min(k * 5, len(self.records))\n",
    "        distances, indices = self.index.search(query_embedding.astype('float32'), search_k)\n",
    "\n",
    "        results = []\n",
    "        for dist, idx in zip(distances[0], indices[0]):\n",
    "            if idx < len(self.records):\n",
    "                record = self.records[idx]\n",
    "                metadata = self.metadata[idx]\n",
    "\n",
    "                if filters and not self._matches_filters(metadata, filters):\n",
    "                    continue\n",
    "\n",
    "                results.append({\n",
    "                    'primary_key': record.primary_key,\n",
    "                    'similarity_score': float(dist),\n",
    "                    'record': record.data\n",
    "                })\n",
    "\n",
    "                if len(results) >= k:\n",
    "                    break\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _matches_filters(self, metadata: Dict, filters: Dict) -> bool:\n",
    "        \"\"\"Check if metadata matches filters\"\"\"\n",
    "        for key, value in filters.items():\n",
    "            if key not in metadata:\n",
    "                continue\n",
    "\n",
    "            meta_value = metadata[key]\n",
    "\n",
    "            # List of values (OR condition)\n",
    "            if isinstance(value, list):\n",
    "                if meta_value not in value:\n",
    "                    return False\n",
    "\n",
    "            # Comparison operators\n",
    "            elif isinstance(value, tuple) and len(value) == 2:\n",
    "                operator, threshold = value\n",
    "                try:\n",
    "                    meta_value = float(meta_value)\n",
    "                    threshold = float(threshold)\n",
    "                except (ValueError, TypeError):\n",
    "                    return False\n",
    "\n",
    "                if operator == '>' and not meta_value > threshold:\n",
    "                    return False\n",
    "                elif operator == '>=' and not meta_value >= threshold:\n",
    "                    return False\n",
    "                elif operator == '<' and not meta_value < threshold:\n",
    "                    return False\n",
    "                elif operator == '<=' and not meta_value <= threshold:\n",
    "                    return False\n",
    "                elif operator == '==' and not meta_value == threshold:\n",
    "                    return False\n",
    "                elif operator == '!=' and not meta_value != threshold:\n",
    "                    return False\n",
    "            else:\n",
    "                # Exact match\n",
    "                if str(metadata[key]).lower() != str(value).lower():\n",
    "                    return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def save(self, path: str):\n",
    "        save_path = Path(path)\n",
    "        save_path.mkdir(parents=True, exist_ok=True)\n",
    "        faiss.write_index(self.index, str(save_path / \"index.faiss\"))\n",
    "        with open(save_path / \"data.pkl\", 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'records': self.records,\n",
    "                'texts': self.texts,\n",
    "                'metadata': self.metadata,\n",
    "                'text_columns': self.text_columns\n",
    "            }, f)\n",
    "\n",
    "    def load(self, path: str):\n",
    "        \"\"\"Load vector store\"\"\"\n",
    "        load_path = Path(path)\n",
    "        self.index = faiss.read_index(str(load_path / \"index.faiss\"))\n",
    "        with open(load_path / \"data.pkl\", 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            self.records = data['records']\n",
    "            self.texts = data['texts']\n",
    "            self.metadata = data['metadata']\n",
    "            self.text_columns = data.get('text_columns')"
   ],
   "id": "7999dbd0dc3b5769"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class UniversalDataIndexer:\n",
    "\n",
    "\n",
    "    def __init__(self, embedding_model: str = 'all-MiniLM-L6-v2'):\n",
    "        self.stores: Dict[str, VectorStore] = {}\n",
    "        self.embedding_model = embedding_model\n",
    "        self.csv_schemas: Dict[str, Dict] = {}\n",
    "\n",
    "    def index_csv(self,\n",
    "                  csv_path: str,\n",
    "                  store_name: str,\n",
    "                  primary_key_column: Optional[str] = None,\n",
    "                  text_columns: Optional[List[str]] = None,\n",
    "                  exclude_columns: Optional[List[str]] = None):\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        if primary_key_column is None:\n",
    "            primary_key_column = df.columns[0]\n",
    "\n",
    "        if exclude_columns:\n",
    "            df = df.drop(columns=exclude_columns, errors='ignore')\n",
    "\n",
    "        self.csv_schemas[store_name] = {\n",
    "            'columns': list(df.columns),\n",
    "            'primary_key': primary_key_column,\n",
    "            'text_columns': text_columns\n",
    "        }\n",
    "\n",
    "        records = []\n",
    "        for _, row in df.iterrows():\n",
    "            data = {}\n",
    "            for col in df.columns:\n",
    "                value = row[col]\n",
    "                data[col] = '' if pd.isna(value) else value\n",
    "\n",
    "            record = GenericRecord(data=data, primary_key=str(row[primary_key_column]))\n",
    "            records.append(record)\n",
    "\n",
    "        store = VectorStore(self.embedding_model)\n",
    "        store.add_records(records, text_columns)\n",
    "        self.stores[store_name] = store\n",
    "\n",
    "        print(f\"✓ Indexed {len(records)} records from '{csv_path}' into '{store_name}' store\")\n",
    "\n",
    "    def search(self, store_name: str, query: str, k: int = 5, filters: Optional[Dict] = None) -> List[Dict]:\n",
    "        \"\"\"Search a vector store\"\"\"\n",
    "        if store_name not in self.stores:\n",
    "            raise ValueError(f\"Store '{store_name}' not found. Available: {list(self.stores.keys())}\")\n",
    "        return self.stores[store_name].search(query, k, filters)\n",
    "\n",
    "    def get_schema(self, store_name: str) -> Dict:\n",
    "        return self.csv_schemas.get(store_name, {})\n",
    "\n",
    "    def list_stores(self) -> List[str]:\n",
    "        return list(self.stores.keys())\n",
    "\n",
    "    def save_all(self, base_path: str = \"./vector_stores\"):\n",
    "        for store_name, store in self.stores.items():\n",
    "            store.save(f\"{base_path}/{store_name}\")\n",
    "\n",
    "        schema_path = Path(base_path) / \"schemas.json\"\n",
    "        schema_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(schema_path, 'w') as f:\n",
    "            json.dump(self.csv_schemas, f, indent=2)\n",
    "\n",
    "        print(f\"✓ Saved all stores to '{base_path}'\")\n",
    "\n",
    "    def load_all(self, base_path: str = \"./vector_stores\"):\n",
    "        \"\"\"Load all stores\"\"\"\n",
    "        base = Path(base_path)\n",
    "\n",
    "        schema_path = base / \"schemas.json\"\n",
    "        if schema_path.exists():\n",
    "            with open(schema_path, 'r') as f:\n",
    "                self.csv_schemas = json.load(f)\n",
    "\n",
    "        for store_dir in base.iterdir():\n",
    "            if store_dir.is_dir():\n",
    "                store_name = store_dir.name\n",
    "                store = VectorStore(self.embedding_model)\n",
    "                store.load(str(store_dir))\n",
    "                self.stores[store_name] = store\n",
    "\n",
    "        print(f\"✓ Loaded {len(self.stores)} stores from '{base_path}'\")\n",
    "\n"
   ],
   "id": "dd8a0c696ae62a25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    indexer = UniversalDataIndexer()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MULTI-MODAL CSV DATA INDEXER\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n--- MENU ---\")\n",
    "        print(\"1. Index a CSV file\")\n",
    "        print(\"2. Search indexed data\")\n",
    "        print(\"3. List indexed stores\")\n",
    "        print(\"4. Save all stores\")\n",
    "        print(\"5. Load saved stores\")\n",
    "        print(\"6. Exit\")\n",
    "\n",
    "        choice = input(\"\\nSelect option (1-6): \").strip()\n",
    "\n",
    "        if choice == '1':\n",
    "            # Index CSV\n",
    "            csv_path = input(\"Enter CSV file path: \").strip()\n",
    "            store_name = input(\"Enter store name: \").strip()\n",
    "            primary_key = input(\"Enter primary key column (or press Enter for auto): \").strip()\n",
    "\n",
    "            try:\n",
    "                indexer.index_csv(\n",
    "                    csv_path=csv_path,\n",
    "                    store_name=store_name,\n",
    "                    primary_key_column=primary_key if primary_key else None\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error: {e}\")\n",
    "\n",
    "        elif choice == '2':\n",
    "            # Search\n",
    "            if not indexer.list_stores():\n",
    "                print(\"✗ No stores indexed yet. Please index a CSV first.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"\\nAvailable stores: {', '.join(indexer.list_stores())}\")\n",
    "            store_name = input(\"Enter store name: \").strip()\n",
    "\n",
    "            if store_name not in indexer.list_stores():\n",
    "                print(f\"✗ Store '{store_name}' not found\")\n",
    "                continue\n",
    "\n",
    "            query = input(\"Enter search query: \").strip()\n",
    "            k = input(\"Number of results (default 5): \").strip()\n",
    "            k = int(k) if k else 5\n",
    "\n",
    "            # Optional filters\n",
    "            use_filters = input(\"Add filters? (y/n): \").strip().lower()\n",
    "            filters = None\n",
    "            if use_filters == 'y':\n",
    "                filter_key = input(\"  Filter column: \").strip()\n",
    "                filter_value = input(\"  Filter value (or >value, <value): \").strip()\n",
    "\n",
    "                if filter_value.startswith('>'):\n",
    "                    filters = {filter_key: ('>', float(filter_value[1:]))}\n",
    "                elif filter_value.startswith('<'):\n",
    "                    filters = {filter_key: ('<', float(filter_value[1:]))}\n",
    "                else:\n",
    "                    filters = {filter_key: filter_value}\n",
    "\n",
    "            try:\n",
    "                results = indexer.search(store_name, query, k, filters)\n",
    "\n",
    "                print(f\"\\n--- RESULTS ({len(results)} found) ---\")\n",
    "                for i, result in enumerate(results, 1):\n",
    "                    print(f\"\\n{i}. {result['primary_key']} (Score: {result['similarity_score']:.3f})\")\n",
    "                    for key, value in list(result['record'].items())[:5]:\n",
    "                        print(f\"   {key}: {value}\")\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error: {e}\")\n",
    "\n",
    "        elif choice == '3':\n",
    "            # List stores\n",
    "            stores = indexer.list_stores()\n",
    "            if not stores:\n",
    "                print(\"\\n✗ No stores indexed yet\")\n",
    "            else:\n",
    "                print(f\"\\n--- INDEXED STORES ({len(stores)}) ---\")\n",
    "                for store in stores:\n",
    "                    schema = indexer.get_schema(store)\n",
    "                    print(f\"\\n{store}:\")\n",
    "                    print(f\"  Records: {len(indexer.stores[store].records)}\")\n",
    "                    print(f\"  Primary Key: {schema['primary_key']}\")\n",
    "                    print(f\"  Columns: {', '.join(schema['columns'][:5])}...\")\n",
    "\n",
    "        elif choice == '4':\n",
    "            # Save\n",
    "            path = input(\"Save path (default: ./vector_stores): \").strip()\n",
    "            path = path if path else \"./vector_stores\"\n",
    "            indexer.save_all(path)\n",
    "\n",
    "        elif choice == '5':\n",
    "            # Load\n",
    "            path = input(\"Load path (default: ./vector_stores): \").strip()\n",
    "            path = path if path else \"./vector_stores\"\n",
    "            try:\n",
    "                indexer.load_all(path)\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error: {e}\")\n",
    "\n",
    "        elif choice == '6':\n",
    "            print(\"\\nGoodbye!\")\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            print(\"✗ Invalid option\")"
   ],
   "id": "ce24aa9fed2103fc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
